<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Aero15x开箱]]></title>
    <url>%2F2018%2F04%2F17%2FAero15x%E5%BC%80%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[之前一直在gs65和aero15x中摇摆不定，在已经付了gs65的定金后还是选择了aero15x。主要原因就是aero15x比较好拆~ 16号凌晨下单，等了一天半到手。先来个箱子图： 全部部件，这个适配器真是大，跟块板砖差不多，另外这个光盘是闹哪样，这个本子有光驱吗？自家的AORUS都用U盘了，赢刃还没跟上，真是无力吐槽。 外观，整机是全金属的，感觉做工还算可以，也能单手开合，就是有点指纹收集器的感觉。个人不喜欢太浮夸的外观，这样感觉刚刚好。 下了个娱乐大师，看一眼配置： 之前看别人的拆机内存是芝奇，显示器是LG的，到我变成金士顿和友达，总有点被缩水的感觉…… 跑个分： 由于有后台，比实际水平大概低一点。 ​ 最后来张图结束。]]></content>
      <categories>
        <category>生活记录</category>
      </categories>
      <tags>
        <tag>Unboxing</tag>
        <tag>Gigabyte</tag>
        <tag>Laptop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络与深度学习]]></title>
    <url>%2F2018%2F04%2F09%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[神经网络是由具有自适应性的简单单元组成的广泛并行互连的网络，他的组织能模拟生物神经系统对真实世界物体所作出的交互反应。它是一门非常重要的机器学习技术，也是深度学习的基础。这篇本来是组会上的要讲的内容，由于老师不喜欢在ppt上写太多文字，所以以ppt为基础，稍微扩充了一下扔博客上了。 感知器 神经网络中最基本的成分即神经元模型。人脑神经元结构如下： 在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时向相连的神经元发送化学物质以改变这些神经元内的电位；如果某神经元的电位超过了阈值，那么它就会被激活，即兴奋起来，向其他神经元发送化学物质。根据上述情形抽象建立一个单层感知器。在该模型中，神经元接收到其他n个神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递。其中输入相当于树突，输出可类比为轴突，计算则认为是细胞体。 对于以上结构，可以看作输入了数值x1和x2，箭头则是与输入相关联的权重w1,w2及偏置b，得到z，函数f是非线性的，称为激活函数。它的作用是将非线性引入到神经元的输出中，以此达到神经元学习非线性表示的目的，满足实际环境的数据要求。 常用的激活函数有Sigmoid函数、tanh函数、ReLU函数等。 Sigmoid函数被表示为：σ(x) = 1 / (1 + exp(−x))，主要把线性函数的输出值缩放到0和1之间。 tanh函数被表示为：tanh(x) = 2σ(2x) − 1，主要把输出值缩放到-1和1之间。 ReLU函数被表示为：f(x) = max(0, x)，其实就是一个分段线性函数，把所有的负值都变为0，而正值不变，这样使得神经网络中的神经元也具有了稀疏激活性。 多层感知器 多层感知器（MLP）包含一个或多个隐藏层（除了一个输入层和一个输出层）。当然目前一般提到多层感知器是指一个隐藏层的，更多隐藏层属于深度学习的范畴。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。 上图是一个多层感知器，它由输入层、隐藏层和输出层组成。实际上，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。 这样就导出了两层神经网络可以做非线性分类的关键–隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。 两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。 反向传播 机器学习模型训练的目的是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y~p~。那么，定义一个损失，我们的目标就是使对所有训练数据的损失和尽可能的小。通常用loss或者coss表示。这里采用loss，即loss = (y~p~ - y)^2^，将先前的神经网络预测的矩阵公式带入到y~p~，那么我们可以把损失写为关于参数的函数，这个函数称之为损失函数。下面的问题就是求：如何优化参数，能够让损失函数的值最小。 此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。 在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，根据误差降低最快的方向调整权重。然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。当然也以同样的方法调整权重。 一般情况下，这种算法需要在整个训练集上多次迭代，直到网络收敛为止。 多层神经网络（深度学习）普通深度网络（DNN） 我们将上面的两层神经网络的输出层变成隐藏层，再新增一个输出层，可以得到下图： 依照这样的方式不断添加，我们可以得到更多层的多层神经网络。多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。 与两层层神经网络不同。多层神经网络中的层数增加了很多。 增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。 更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。 更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量去拟合真正的关系。 通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。当然相应的参数也更加难以确定，更难训练。 = =没写完，改天补上 参考资料：《机器学习》(周志华) 、神经网络浅讲：从神经元到深度学习、如何简单形象又有趣地讲解神经网络是什么？、卷积：如何成为一个很厉害的神经网络]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FP-growth算法]]></title>
    <url>%2F2018%2F02%2F06%2FFP-growth%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[你用过搜索引擎吗？输入一个单词或者一个单词的一部分，搜索引擎就会自动补全查询词项。用户甚至事先都不知道搜索引擎推荐的东西是否存在，反而会去查找推荐词项。那么这些推荐词项是如何被搜索引擎找到的？那是因为研究人员使用了FP-growth算法，它是基于Apriori构建的，但完成相同任务时将数据集存储在一个特定的称作FP树的结构之后发现频繁项集或频繁项对，即常在一起出现的元素项的集合FP树。这是一种比Apriori执行速度更快的算法，能高效地发现频繁项集，但不能用于发现关联规则。 FP-growth算法只需要对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁，因此FP-growth算法的速度比Apriori算法快。在小规模数据上，这不是什么问题，但当处理更大数据集时，就会产生较大问题。FP-growth算法只会扫描数据集两次，它不会生成候选项集，其发现频繁项集的基本过程如下： 构建FP树 从FP树中挖掘频繁项集 FP-growth原理 FP-growth算法将数据存储在一种成为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一颗FP树与计算机科学中其他树结构类似，但它通过链接（link）来连接相似元素，被连起来的元素可以看成一个链表。 下面用一个例子来说明，给出一组数据如下： 事物ID 事物中的元素项 001 r,z,h,j,p 002 z,y,x,w,v,u,t,s 003 z 004 r,x,n,o,s 005 y,r,x,z,q,t,p 006 y,z,x,e,q,s,t,m 由此可以生成一棵FP树： 仔细来看这棵FP树，同搜索树不同，一个元素项可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中单个元素及其在序列中的出现次数，路径会给出该序列的出现次数。相似项之间的链接，即节点链接（node link），用于快速发现相似项的位置。 上图中，元素项z出现了5次，集合{r, z}出现了1次。于是可以知道：z一定是自己本身或者和其他符号一起出现了4次。再看看其他可能，集合{t, s, y, x, z}出现了2次，集合{t, r, y, x, z}出现了1次。元素项z的右边是5，表示z出现了5次，其中刚才已经给出了4次出现，所以它一定单独出现过1次。005号记录是{y, r, x, z, q, t, p}，那么q、p去哪儿了呢？ 这里使用第11章给出的支持度定义，该指标对应一个最小阈值，低于最小阈值的元素项被认为是不频繁的。若将最小支持度设为3，然后应用频繁项分析算法，就会获得出现3次或3次以上的项集。图中FP树的最小支持度是3，因此p、q并没有出现在树中。 FP-growth算法的工作流程：首先构建FP树，然后利用它来挖掘频繁项集。为了构建FP树，需要对原始数据集扫描两遍。第一遍对所有元素项的出现次数进行计数。记住Apriori原理：如果某元素是不频繁的，那么包含该元素的超集也是不频繁的，所以就不需要考虑超集。数据库的第一遍扫描用来统计出现的频率，第二遍扫描中只考虑那些频繁元素。 构建FP树 首先先用一个容器来保存FP树。 创建FP树的数据结构 创造一个类保存树的每个节点，创建文件fpGrowth.py并加入以下代码： 123456789101112131415161718192021class treeNode : def __init__(self, nameValue, numOccur, parentNode) : # 节点名称 self.name = nameValue self.count = numOccur # 用于链接相似的元素项 self.nodeLink = None # 当前节点的父节点 self.parent = parentNode # 用于存放节点的子节点 self.children = &#123;&#125; # 对count变量增加给定值 def inc(self, numOccur) : self.count += numOccur # 将树以文本的形式显示 def disp(self, ind=1) : print(' '*ind, self.name, ' ', self.count) for child in self.children.values() : child.disp(ind+1) 上面的程序给出了FP树中节点的类定义。类中包含用于存放节点名字的变量和1个计数值，nodeLink变量用于链接相似的元素项（参考上图中的虚线）。类中还使用了父变量parent来指向当前节点的父节点。通常情况并不需要这个变量，因为通常是从上往下迭代访问节点的。如果需要根据给定叶子节点上溯整棵树，这时候就需要指向父节点的指针。最后，类中还包含一个空字典变量，用于存放节点的子节点。 运行一下如下代码： 1234567891011121314&gt;&gt;&gt; import fpGrowth&gt;&gt;&gt; rootNode = fpGrowth.treeNode('pyramid',9,None)# 已创建一个单节点，接下来增加一个子节点&gt;&gt;&gt; rootNode.children['eye'] = fpGrowth.treeNode('eye',13,None)# 显示子节点&gt;&gt;&gt; rootNode.disp() pyramid 9 eye 13# 再添加一个节点&gt;&gt;&gt; rootNode.children['phoenix']=fpGrowth.treeNode('phoenix',3,None)&gt;&gt;&gt; rootNode.disp() pyramid 9 eye 13 phoenix 3 构建FP树 除上图给出的FP树之外，还需要一个头指针表来指向给定类型的第一个实例。利用头指针表，可快速访问FP树中一个给定类型的所有元素。下图给出了一个头指针表的示意图。 这里使用一个字典作为数据结构来保存头指针表。除了存放指针外，头指针表还可以用来保存FP树中每类元素的总数。 第一次遍历数据集会获得每个元素项的出现频率。接着去掉不满足最小支持度的元素项，再来构建FP树。在构建时，读入每个项集并将其添加到一条已经存在的路径中。如果该路径不存在，则创建一条新路径。每个事务就是一个无序集合。假设有集合{z, x, y}和{y, z, r}，那么在FP树中，相同项会只表示一次。为了解决此问题，在将集合添加到树之前，需要对每个集合进行排序。排序基于元素项的绝对出现频率来进行。使用上图中头指针节点值，对前表中数据进行过滤、重排序后的数据显示如下。 事务ID 事务中的元素项 过滤及重排序后的事务 001 r, z, h, j, p z, r 002 z, y, x, w, v, u, t, s z, x, y, s, t 003 z z 004 r, x, n, o, s x, s, r 005 y, r, x, z, q, t, p z, x, y, r, t 006 y, z, x, e, q, s, t, m z, x, y, s, t 在对事务记录过滤和排序之后，就可以构建FP树了。从空集（符号为∅）开始，向其中不断添加频繁项集。过滤、排序后的事务依次添加到树中，如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝。对上表前两条事务进行添加的过程显示如下。 接下来通过代码实现上述过程。在fpGrowth.py加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# FP树构建函数# 使用数据集以及最小支持度作为参数来构建FP树，树构建过程会遍历数据集两次def createTree(dataSet, minSup=1) : headerTable = &#123;&#125; # 第一次遍历扫描数据集并统计每个元素项出现的频度，这些信息被保存在头指针中 for trans in dataSet : for item in trans : headerTable[item] = headerTable.get(item, 0) + dataSet[trans] # 接着扫描头指针表删除那些出现次数小于minSup的项，由于字典不能在遍历中修改，转成集合 for k in list(headerTable.keys()): if headerTable[k] &lt; minSup : del(headerTable[k]) freqItemSet = set(headerTable.keys()) # 如果所有项都不频繁，无需下一步处理 if len(freqItemSet) == 0 : return None, None # 对头指针表稍加扩展以便可以保存计数值及指向每种类型第一个元素项的指针 for k in headerTable : headerTable[k] = [headerTable[k], None] # 创建只包含空集合的根节点 retTree = treeNode('Null Set', 1, None) for tranSet, count in dataSet.items() : localD = &#123;&#125; # 根据全局频率对每个事务中的元素进行排序 for item in tranSet : if item in freqItemSet : localD[item] = headerTable[item][0] if len(localD) &gt; 0 : orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p : p[1], reverse=True)] # 排序后，调用updateTree()方法 updateTree(orderedItems, retTree, headerTable, count) return retTree, headerTable# 为了让FP树生长，需调用updateTree函数。def updateTree(items, inTree, headerTable, count) : # 该函数首先测试事务中的第一个元素项是否作为子节点存在。 if items[0] in inTree.children : # 如果存在，则更新该元素项的计数 inTree.children[items[0]].inc(count) else : # 如果不存在，则创建一个新的treeNode并将其作为一个子节点添加到树中，这时，头指针表也要更新以指向新的节点。 inTree.children[items[0]] = treeNode(items[0], count, inTree) if headerTable[items[0]][1] == None : headerTable[items[0]][1] = inTree.children[items[0]] else : # 更新头指针表需要调用函数updateHeader updateHeader(headerTable[items[0]][1], inTree.children[items[0]]) # updateTree()完成的最后一件事是不断迭代调用自身，每次调用时会去掉列表中的第一个元素 if len(items) &gt; 1 : updateTree(items[1::], inTree.children[items[0]], headerTable, count)# 确保节点链接指向树中该元素项的每一个实例，从头指针的nodeLink开始，一直沿着nodeLink直到到达链表末尾。# 当处理树的时候，一种自然的反应就是迭代完整每一件事。当以相同方式处理链表时可能会遇到一些问题，# 原因是如果链表很长可能会遇到迭代调用的次数限制def updateHeader(nodeToTest, targetNode) : while (nodeToTest.nodeLink != None) : nodeToTest = nodeToTest.nodeLink nodeToTest.nodeLink = targetNode# 载入数据集def loadSimpDat() : simpDat = [ ['r', 'z', 'h', 'j', 'p' ], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's' ], ['z' ], ['r', 'x', 'n', 'o', 's' ], ['y', 'r', 'x', 'z', 'q', 't', 'p' ], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm' ] ] return simpDat# 从列表向字典的类型转换def createInitSet(dataSet) : retDict = &#123;&#125; for trans in dataSet : retDict[frozenset(trans)] = 1 return retDict 运行结果： 123456789101112131415161718192021222324&gt;&gt;&gt; import fpGrowth&gt;&gt;&gt; simpDat = fpGrowth.loadSimpDat()&gt;&gt;&gt; simpDat[['r', 'z', 'h', 'j', 'p'], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'], ['z'], ['r', 'x', 'n', 'o', 's'], ['y', 'r', 'x', 'z', 'q', 't', 'p'], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]&gt;&gt;&gt; initSet = fpGrowth.createInitSet(simpDat)&gt;&gt;&gt; initSet&#123;frozenset(&#123;'j', 'z', 'h', 'p', 'r'&#125;): 1, frozenset(&#123;'u', 's', 'x', 'w', 'y', 'v', 'z', 't'&#125;): 1, frozenset(&#123;'z'&#125;): 1, frozenset(&#123;'s', 'x', 'o', 'n', 'r'&#125;): 1, frozenset(&#123;'q', 'x', 'y', 'z', 't', 'p', 'r'&#125;): 1, frozenset(&#123;'s', 'e', 'q', 'x', 'y', 'z', 'm', 't'&#125;): 1&#125;&gt;&gt;&gt; myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 3)&gt;&gt;&gt; myFPtree.disp() Null Set 1 z 5 r 1 x 3 s 2 y 2 t 2 y 1 t 1 r 1 x 1 s 1 r 1 上面给出的是元素项及其对应的频率计数值，其中每个缩进表示所处的树的深度。 从一棵FP树中挖掘频繁项集 有了FP树之后，就可以抽取频繁项集了。这里的思路与Apriori算法大致类似，首先从单元素项集合开始，然后在此基础上逐步构建更大的集合。当然这里将利用FP树来做实现上述过程，不再需要原始数据集了。从FP树中抽取频繁项集的三个基本步骤如下： 从FP树中获得条件模式基； 利用条件模式基，构建一个条件FP树； 迭代重复步骤1、2，直到树包含一个元素项为止。 重点关注第1步，即寻找条件模式基的过程。之后，为每一条件模式基创建对应的条件FP树。最后需构造少许代码来封装上述两个函数，并从FP树中获得频繁项集。 抽取条件模式基 首先从之前保存在头指针中的单个频繁元素项开始。对于每一个元素项，获得其对应的条件模式(conditional pattern base)。条件模式基是以所查找元素项为结尾的路径集合。每一条路径其实都是一条前缀路径(prefix path)。简而言之，一条前缀路径是介于所查找元素项与树根节点之间的所有内容。 回到图2中，符号r的前缀路径是{x, s}、{z, x, y}和{z}。每条前缀路径都与一个计数值关联。该计数值等于起始元素项的计数值，该计数值给了每条路径上r的数目。下表列出了上例当中每一个频繁项的所有前缀路径： 频繁项 前缀路径 z {}5 r {x, s}1, {z, x, y}1, {z}1 x {z}3, {}1 y {z, x}3 s {z, x, y}2, {x}1 t {z, x, y, s}2, {z, x, y, r}1 前缀路径被用于构建条件FP树，但是暂时先不需要考虑这件事。为了获得这些前缀路径，可以对树进行穷举式搜索，直到获得想要的频繁项为止，或使用一个更有效的方法来加速搜索过程。可以利用先前创建的头指针来得到一种更有效的方法。头指针表包含相同类型元素链表的起始指针。一旦到达了每一个元素项，就可以上溯这棵树直到根节点为止。下面代码给出了如何发现前缀路径，将其添加到文件fpGrowth.py中。 1234567891011121314151617def ascendTree(leafNode, prefixPath) : # 迭代上溯整棵树 if leafNode.parent != None : prefixPath.append(leafNode.name) ascendTree(leafNode.parent, prefixPath)# 遍历链表直到到达结尾。每遇到一个元素项都会调用ascendTree()来上溯FP树，并收集所有遇到的元素项的名称。# 该列表返回之后添加到条件模式基字典condPats中def findPrefixPath(basePat, treeNode) : condPats = &#123;&#125; while treeNode != None : prefixPath = [] ascendTree(treeNode, prefixPath) if len(prefixPath) &gt; 1 : condPats[frozenset(prefixPath[1:])] = treeNode.count treeNode = treeNode.nodeLink return condPats 运行结果 123456&gt;&gt;&gt; fpGrowth.findPrefixPath('x', myHeaderTab['x'][1])&#123;frozenset(&#123;'z'&#125;): 3&#125;&gt;&gt;&gt; fpGrowth.findPrefixPath('z', myHeaderTab['z'][1])&#123;&#125;&gt;&gt;&gt; fpGrowth.findPrefixPath('r', myHeaderTab['r'][1])&#123;frozenset(&#123;'z'&#125;): 1, frozenset(&#123;'s', 'x'&#125;): 1, frozenset(&#123;'z', 't', 'y', 'x'&#125;): 1&#125; 创建条件FP树 对于每个频繁项，都要创建一棵条件FP树。我们会为z、x以及其他频繁项构建条件树。可以使用刚才发现的条件模式基作为输入数据，并通过相同的建树代码来构建这些树。然后，我们会递归地发现频繁项、发现条件模式基，以及发现另外的条件树。举个例子，假定为频繁项 t 创建一个条件FP树，然后对{t, y}、{t, x}、……重复该过程。元素项t的条件FP树的构建过程如下所示。 上图中最初树以空集作为根节点，接着，原始的集合{y, x, s, z}中的集合{y, x, z}被添加进来。因为不满足最小支持度要求，字符s并没有加入进来。类似地，{y, x, z}也从原始集合{y, x, r, z}中添加进来。 元素项s、r是条件模式基的一部分，但它们并不属于条件FP树。单独来看它们都是频繁项，但是在t的条件树中，它们却不是频繁的，也就是说{t, r}、{t, s}是不频繁的。 接下来，对集合{t, z}、{t, x}、{t, y}来挖掘对应的条件树。这会产生更复杂的频繁项集。该过程重复进行，直到条件树中没有元素为止，然后就可以停止了。实现代码很直观，使用一些递归加上之前写的代码即可。具体如下： 12345678910111213141516171819def mineTree(inTree, headerTable, minSup, preFix, freqItemList) : # 对头指针表中元素项按照其出现频率进行排序，默认是从小到大 bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p:p[1][0])] # 默认是从小到大，下面过程是从头指针的底端开始 for basePat in bigL : newFreqSet = preFix.copy() newFreqSet.add(basePat) # 将每个频繁项添加到频繁项集列表freqItemList中 freqItemList.append(newFreqSet) # 使用findPrefixPath()创建条件基 condPattBases = findPrefixPath(basePat, headerTable[basePat][1]) # 将条件基condPattBases作为新数据集传递给createTree()函数 # 这里为函数createTree()添加足够的灵活性，确保它可以被重用于构建条件树 myCondTree, myHead = createTree(condPattBases, minSup) # 如果树中有元素项的话，递归调用mineTree()函数 if myHead != None : print('conditional tree for: ', newFreqSet) myCondTree.disp() mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList) 效果如下： 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; freqItems = []&gt;&gt;&gt; fpGrowth.mineTree(myFPtree, myHeaderTab, 3, set([]), freqItems)conditional tree for: &#123;'s'&#125; Null Set 1 x 3conditional tree for: &#123;'y'&#125; Null Set 1 z 3 x 3conditional tree for: &#123;'y', 'x'&#125; Null Set 1 z 3conditional tree for: &#123;'t'&#125; Null Set 1 z 3 y 3 x 3conditional tree for: &#123;'t', 'y'&#125; Null Set 1 z 3conditional tree for: &#123;'t', 'x'&#125; Null Set 1 y 3 z 3conditional tree for: &#123;'z', 't', 'x'&#125; Null Set 1 y 3conditional tree for: &#123;'x'&#125; Null Set 1 z 3# 检查返回的项集是否与条件树匹配&gt;&gt;&gt; freqItems[&#123;'r'&#125;, &#123;'s'&#125;, &#123;'s', 'x'&#125;, &#123;'y'&#125;, &#123;'y', 'z'&#125;, &#123;'y', 'x'&#125;, &#123;'z', 'y', 'x'&#125;, &#123;'t'&#125;, &#123;'t', 'z'&#125;, &#123;'t', 'y'&#125;, &#123;'t', 'y', 'z'&#125;, &#123;'t', 'x'&#125;, &#123;'t', 'y', 'x'&#125;, &#123;'z', 't', 'x'&#125;, &#123;'z', 't', 'y', 'x'&#125;, &#123;'x'&#125;, &#123;'z', 'x'&#125;, &#123;'z'&#125;] 示例：从新闻网站点击流中挖掘 下列文件kosarak.dat中包含了将近100万条记录。该文件中的每一行包含某个用户浏览过的新闻报道。一些用户只看过一篇报道，而有些用户看过2498篇报道。用户和报道被编码成整数，所以查看频繁项集很难得到更多的东西，但该数据对于展示FP-growth算法的速度十分有效。 数据前几行如下： 12345678910111213141 2 314 5 6 71 89 1011 6 12 13 14 15 161 3 717 1811 6 19 20 21 22 23 241 25 326 311 27 6 3 28 7 29 30 31 32 33 34 35 36 376 2 3839 11 27 1 40 6 41 42 43 44 45 46 47 3 48 7 49 50 51 运用FP-growth算法： 123456789101112131415161718192021222324252627# 导入数据&gt;&gt;&gt; parsedDat = [line.split() for line in open('kosarak.dat').readlines()]# 初始化数据&gt;&gt;&gt; initSet = fpGrowth.createInitSet(parsedDat)# 构建FP树，寻找至少被10万人浏览过的新闻报道&gt;&gt;&gt; myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 100000)# 创建空列表保存频繁项集&gt;&gt;&gt; myFreqList = []&gt;&gt;&gt; fpGrowth.mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreqList)conditional tree for: &#123;'1'&#125; Null Set 1 6 107404conditional tree for: &#123;'3'&#125; Null Set 1 6 186289 11 117401 11 9718conditional tree for: &#123;'11', '3'&#125; Null Set 1 6 117401conditional tree for: &#123;'11'&#125; Null Set 1 6 261773&gt;&gt;&gt; len(myFreqList)9&gt;&gt;&gt; myFreqList[&#123;'1'&#125;, &#123;'1', '6'&#125;, &#123;'3'&#125;, &#123;'11', '3'&#125;, &#123;'11', '6', '3'&#125;, &#123;'6', '3'&#125;, &#123;'11'&#125;, &#123;'11', '6'&#125;, &#123;'6'&#125;] 小结 FP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。Apriori算法产生候选项集，然后扫描数据集来检查它们是否频繁。由于只对数据集扫描两次，因此FP-growth算法执行更快。在FP-growth算法中，数据集存储在FP树中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，直到FP树只包含一个元素为止。 可以使用FP-growth算法在多种文本文档中查找频繁单词。对Twitter源上的某个话题应用FP-growth算法，可以得到一些有关该话题的摘要信息。频繁项集生成还可以用于购物交易、医学诊断和大气研究等。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>Association Analysis</tag>
        <tag>Python</tag>
        <tag>FP-growth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录实用的python库]]></title>
    <url>%2F2018%2F02%2F03%2F%E8%AE%B0%E5%BD%95%E5%AE%9E%E7%94%A8%E7%9A%84python%E5%BA%93%2F</url>
    <content type="text"><![CDATA[本文专门用来记录一下python中一些好用方法/库，可以在日常使用中提高效率。 进度条 在爬虫和机器学习等工作中，可能需要有一个进度条能够反馈当前程序运行速度或者进度，可以考虑用以下方法实现： ShowProcess类 在网上找到别人写的一个方法如下： 1234567891011121314151617181920212223242526# 建立一个ShowProcess类class ShowProcess(): i = 0 max_steps = 0 max_arrow = 50 def __init__(self, max_steps): self.max_steps = max_steps self.i = 0 def show_process(self, i=None): if i is not None: self.i = i else: self.i += 1 num_arrow = int(self.i * self.max_arrow / self.max_steps) num_line = self.max_arrow - num_arrow percent = self.i * 100.0 / self.max_steps process_bar = '[' + '&gt;' * num_arrow + '-' * num_line + ']'\ + '%.2f' % percent + '%' print('\r',process_bar,end='',flush=True) def close(self, words='done'): print('') print(words) self.i = 0 使用示例： 12345678910111213if __name__=='__main__': max_steps = 100 process_bar = ShowProcess(max_steps) for i in range(max_steps): process_bar.show_process() time.sleep(0.05) process_bar.close()# 效果如下 &gt;&gt;&gt; [&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00%&gt;&gt;&gt; done 在命令窗口下使用正常，但是在IDLE和Spyder中显示存在问题，考虑使用其他方法。 tqdm安装1pip install tqdm 使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 最基本的用法import timefrom tqdm import tqdmfor i in tqdm(range(9)): time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:01&lt;00:00, 9.79it/s]# trange类似于tqdmimport timefrom tqdm import trangefor i in trange(10): time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:01&lt;00:00, 9.79it/s]# 传入listimport timefrom tqdm import tqdmpbar = tqdm([1,2,3,4,5,6,7,8,9,10]) for char in pbar: pbar.set_description("Processing %s" % char) time.sleep(0.1)# 效果如下&gt;&gt;&gt; Processing 10: 100%|██████████| 10/10 [00:01&lt;00:00, 9.49it/s] # 手动控制更新import timefrom tqdm import tqdmwith tqdm(total=10) as pbar: for i in range(10): pbar.update(1) time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:00&lt;00:00, 10.10it/s]# 也可以这样import timefrom tqdm import tqdmpbar = tqdm(total=10) for i in range(10): pbar.update(1) time.sleep(0.1)pbar.close() # 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:00&lt;00:00, 10.10it/s] 在Spyder下正常了，然而在命令窗口有问题。 重试 进行爬虫的时候，很容易因为网络问题导致失败，这里有2个库可以很轻松地实现这个功能。 retry安装1pip install retry 使用 只需要在函数定义前加上@retry就行了。 1234567891011121314151617from retry import retry@retry()def make_trouble(): '''重试直到成功'''@retry(ZeroDivisionError, tries=3, delay=2)def make_trouble(): '''出现ZeroDivisionError时重试, 重试3次，每次间隔2秒'''@retry((ValueError, TypeError), delay=1, backoff=2)def make_trouble(): '''出现ValueError或TypeError时重试, 每次间隔1, 2, 4, 8, ...秒'''@retry((ValueError, TypeError), delay=1, backoff=2, max_delay=4)def make_trouble(): '''出现ValueError或TypeError时重试, 每次间隔1, 2, 4, 4, ...秒，最高间隔为4秒'''@retry(ValueError, delay=1, jitter=1)def make_trouble(): '''出现ValueError时重试,每次间隔1, 2, 3, 4, ... 秒''' Tenacity 使用类似于retry。同样只需要在函数定义前加上@retry就行了。 安装1pip install tenacity 使用12345678910111213141516171819from tenacity import retry, retry_if_exception_type, wait_fixed, stop_after_attempt, stop_after_delay,@retry()def make_trouble(): '''重试直到成功'''@retry(retry=retry_if_exception_type(ZeroDivisionError), wait=wait_fixed(2), stop=stop_after_attempt(3))def make_trouble(): '''出现ZeroDivisionError时重试, 重试3次，每次间隔2秒'''@retry(stop=(stop_after_delay(10) | stop_after_attempt(5)))def make_trouble(): '''重试10秒或者5次''' @retry(wait=wait_random(min=1, max=2))def make_trouble(): '''重试间隔在随机1-2秒'''@retry(wait=wait_chain(*[wait_fixed(3) for i in range(3)] + [wait_fixed(7) for i in range(2)] + [wait_fixed(9)]))def make_trouble(): '''前三次重试每次间隔3秒，接下来2次间隔7秒，之后重试间隔9秒''' 异步 待填坑……]]></content>
      <categories>
        <category>效率工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori算法]]></title>
    <url>%2F2018%2F01%2F30%2FApriori%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[很多人都听说过“尿布和啤酒”的故事：据说，美国中西部的一家连锁店发现，男人们去超市买尿布的同时，往往会顺便给自己购买啤酒。由此，卖场开始把啤酒和尿布摆放在相同区域，让男人可以同时找到这两件商品，从而获得了很好的销售收入。虽然并没有商店真的把这两样东西放在一起，但是很多商家确实将大家经常购买的物品放在一起捆绑销售以鼓励大家购买。那么我们如何在繁杂的数据发现这些隐含关系呢？这就需要关联分析（association analysis），本文所讨论的Apriori便是其中一种关联分析算法。 基本概念 关联分析是一种在大规模数据集中寻找有趣关系的任务。这些关系有两种形式：频繁项集、关联规则。频繁项集（frequent item sets）是经常出现在一块的物品的集合；关联规则（association rules）暗示两种物品之间可能存在很强的关系。 以下是某个杂货店的交易清单： 交易号码 商品 0 豆奶，莴苣 1 莴苣，尿布，葡萄酒，甜菜 2 豆奶，尿布，葡萄酒，橙汁 3 莴苣，豆奶，尿布，葡萄酒 4 莴苣，豆奶，尿布，橙汁 频繁项集：经常出现在一起的物品集合，如｛葡萄酒，尿布，豆奶｝就是一个频繁项集。 支持度（support）：如何有效定义频繁？其中最重要的两个概念是支持度和可信度。一个项集的支持度被定义为数据集中包含该项集的记录所占的比例。还是上面的例子，豆奶在5条交易中出现了4次，因此｛豆奶｝的支持度为4/5，同理可知，｛豆奶，尿布｝的支持度为3/5。我们可以定义一个最小支持度，从而只保留满足最小支持度的项集。 可信度或置信度（confidence）：是针对一条关联规则来定义的。例如：我们要讨论｛尿布｝→｛葡萄酒｝的关联规则，它的可信度被定义为“支持度（｛尿布，葡萄酒｝） / 支持度（｛尿布｝）”。因为｛尿布, 葡萄酒｝的支持度为3/5，｛尿布｝的支持度为4/5，所以“尿布→葡萄酒”的可信度为3/4=0.75。 Apriori原理 假设我们在经营一家商品种类并不多的杂货店，我们对那些经常一起被购买的商品很感兴趣。我们只有4种商品：商品0，商品1，商品2和商品3。那么所有可能被一起购买的商品组合有哪些？下图显示了物品之间所有可能的组合。 如何对一条给定的集合，如{0,3}，来计算其支持度？通常我们遍历每条记录并检查该记录包含0和3，如果记录确实包含两项，那么就增加总计数值。在扫描完每条数据后，使用统计的总数除以总交易记录数，就可以得到支持率。同样地，如果要获得每种可能集合的支持度就要多次重复上述过程。对于包含4种物品的集合，需要遍历数据15次。而随着物品数目的增加，遍历次数会急剧增长。对于包含N中物品的数据集共有2N−1中项集组合，对于只出售100中商品的商店也会有1.26×1030中可能的项集组合。对于现代计算机，需要很长的时间才能完成运算。 Apriori原理可以帮助我们减少感兴趣的项集。Apriori原理是指如果某个项集是频繁的，那么它的所有子集也是频繁的。反过来，如果一个项集是非频繁集，那么它的所有超集也是非频繁的。 上述例子中，已知阴影项集{2，3}是非频繁的。利用这个知识，我们就知道项集{0,2,3} ，{1,2,3}以及{0,1,2,3}也是非频繁的。这也就是说，一旦计算出了{2,3}的支持度，知道它是非频繁的之后，就不需要再计算{0,2,3}、{1,2,3}和{0,1,2,3}的支持度，因为我们知道这些集合不会满足我们的要求。使用该原理就可以避免项集数目的指数增长，从而在合理时间内计算出频繁项集。 Apriori算法 发现频繁项集的过程如上图所示： 由数据集生成候选项集C1（1表示每个候选项仅有一个数据项）；再由C1通过支持度过滤，生成频繁项集L1（1表示每个频繁项仅有一个数据项）。 将L1的数据项两两拼接成C2。 从候选项集C2开始，通过支持度过滤生成L2。L2根据Apriori原理拼接成候选项集C3；C3通过支持度过滤生成L3……直到Lk中仅有一个或没有数据项为止。 回到上面的杂货店例子，令最小支持度为0.4，结果如下图： 值得注意的是L3到C4这一步并没有得到候选项集，这是由于Apriori算法由两部分组成（在这里假定购买商品是有顺序的）。 连接：对K-1项集中的每个项集中的项排序，只有在前K-1项相同时才将这两项合并，形成候选K项集（因为必须形成K项集，所以只有在前K-1项相同，第K项不相同的情况下才合并。） 剪枝：对于候选K项集，要验证所有项集的所有K-1子集是否频繁（是否在K-1项集中），去掉不满足的项集，就形成了K项集。比如C4连接的｛尿布，莴苣，葡萄酒，豆奶｝的子集｛莴苣，葡萄酒，豆奶｝不存在于L3，因此要去掉。 实现Apriori代码 根据以上原理构造数据集扫描的Python代码，其伪代码大致如下： 1234567对数据集中的每条交易记录tran对每个候选项集can : 检查一下can是否是tran的子集 : 如果是，则增加can的计数值对每个候选项集 :如果其支持度不低于最小值，则保留该项集返回所有频繁项集列表 建立辅助函数： 12345678910111213141516171819202122232425262728293031323334353637383940# 创建一个简单的测试数据集def loadDataSet() : return [[1,3,4], [2,3,5], [1,2,3,5], [2,5]]# 构建集合C1，C1是大小为1的所有候选项集的集合。def createC1(dataSet) : # C1是空列表，用来存储所有不重复的项值。如果某个物品项没有在C1中出现，则将其添加到C1中。 # 这里并不是简单地每个物品项，而是添加只包含该物品项的一个列表。Python不能创建只有一个整 # 数的集合，因此这里实现必须使用列表 C1 = [] for transaction in dataSet : for item in transaction : if not [item] in C1 : C1.append([item]) C1.sort() # frozenset是指被“冰冻”的集合，就是说它们是不可改变 return list(map(frozenset,C1)) # D: 数据集# Ck: 候选项集列表# minSupport: 感兴趣集的最小支持度minSupport# 该函数会返回一个包含支持度的字典以备后用def scanD(D, Ck, minSupport) : ssCnt = &#123;&#125; for tid in D : for can in Ck : if can.issubset(tid) : if not can in ssCnt: ssCnt[can]=1 else : ssCnt[can] += 1 numItems = float(len(D)) retList = [] supportData = &#123;&#125; for key in ssCnt : # 计算所有项集的支持度 support = ssCnt[key]/numItems if support &gt;= minSupport : # 在列表的首部插入新的集合 retList.insert(0, key) supportData[key] = support return retList, supportData 保存为apriori.py，运行效果如下： 123456789101112131415161718&gt;&gt;&gt; import apriori# 导入数据集&gt;&gt;&gt; dataSet = apriori.loadDataSet()&gt;&gt;&gt; dataSet[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]# 构建第一个候选项集集合C1&gt;&gt;&gt; C1 = apriori.createC1(dataSet)&gt;&gt;&gt; C1[frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])]# 构建集合表示的数据集D&gt;&gt;&gt; D = list(map(set, dataSet))&gt;&gt;&gt; D[&#123;1, 3, 4&#125;, &#123;2, 3, 5&#125;, &#123;1, 2, 3, 5&#125;, &#123;2, 5&#125;]# 去掉不满足最小支持度的项集，0.5为最小支持度&gt;&gt;&gt; L1, suppData0 = apriori.scanD(D, C1, 0.5)# 下面四个项集构成了L1列表，该列表中每个单物品项集至少出现在50%以上的记录中&gt;&gt;&gt; L1[frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])] 整个Apriori算法的伪代码如下： 1234当集合中项的个数大于0时 构建一个k个项组成的候选项集的列表 检查数据以确认每个项集都是频繁的 保留频繁项集并构建k+1项组成的候选项集的列表 将如下算法代码加入apriori.py： 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建候选项集Ck# Lk，频繁项集列表# k，项集元素的个数def aprioriGen(Lk, k) : # create Ck # 创建一个空列表 retList = [] # 计算Lk中的元素 lenLk = len(Lk) for i in range(lenLk) : for j in range(i+1, lenLk) : # 当前k-2个项相同时，将两个集合合并 L1 = list(Lk[i])[:k-2] L2 = list(Lk[j])[:k-2] L1.sort() L2.sort() if L1==L2 : # python中集合的并操作对应的操作符为| retList.append(Lk[i] | Lk[j]) return retList# dataSet，数据集# minSupport，支持度# 此函数会生成候选项集的列表def apriori(dataSet, minSupport = 0.5) : C1 = createC1(dataSet) # map函数将set()映射到dataSet列表中的每一项 D = list(map(set, dataSet)) L1, supportData = scanD(D, C1, minSupport) # 将L1放入L列表中 L = [L1] k = 2 # while循环将L2, L3, L4, ... 放入L列表中，直到下一个大的项集为空 while (len(L[k-2]) &gt; 0) : # 调用aprioriGen()创建候选项集Ck Ck = aprioriGen(L[k-2], k) # 扫描数据集，从Ck得到Lk Lk, supK = scanD(D, Ck, minSupport) supportData.update(supK) L.append(Lk) k += 1 return L, supportData 上面的k-2可能会令人困惑，接下来讨论其细节。当利用{0}、{1}、{2}构建{0,1}、{0,2}、{1,2}时，实际上是将单个项组合到一块。现在如果想利用{0,1}、{0,2}、{1,2}来创建三元素项集，应该怎么做？如果将每两个集合合并，就会得到{0,1,2}、{0,1,2}、{0,1,2}。也就是同样的结果集合会重复3次。接下来需要扫描三元素项集列表来得到非重复结果，我们要做的是确保遍历列表的次数最少。现在，如果比较集合{0,1}、{0,2}、{1,2}的第一个元素并只对第一个元素相同的集合求并操作，又会得到什么结果？{0,1,2}，而且只有一次操作，这样就不用遍历列表来寻找非重复值了。 保存后运行效果如下： 1234567891011121314151617181920212223&gt;&gt;&gt; L, supportData = apriori.apriori(dataSet)&gt;&gt;&gt; L[[frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])], [frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5]), frozenset([1, 3])], [frozenset([2,3, 5])], []]# L包含满足最小支持度为0.5的频率项集列表，下面看一下具体值：&gt;&gt;&gt; L[0][frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])]&gt;&gt;&gt; L[1][frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5]), frozenset([1, 3])]&gt;&gt;&gt; L[2][frozenset([2, 3, 5])]&gt;&gt;&gt; L[3][]# 每个项集都是在函数apriori()中调用函数aprioriGen()来生成的。下面看一下aprioriGen()函数的工作流程：&gt;&gt;&gt; apriori.aprioriGen(L[0], 2)[frozenset([2, 5]), frozenset([3, 5]), frozenset([1, 5]), frozenset([2, 3]), frozenset([1, 2]), frozenset([1, 3])]# 这里的6个集合是候选项集Ck中的元素。其中4个集合在L[1]中，剩下2个集合被函数scanD()过滤掉。# 下面再尝试70%的支持度：&gt;&gt;&gt; L,support = apriori.apriori(dataSet, minSupport=0.7)&gt;&gt;&gt; L[[frozenset([5]), frozenset([2]), frozenset([3])], [frozenset([2, 5])], []] 从频繁项集中挖掘关联规则 关联分析的两个重要目标是发现频繁项集与关联规则。要找到关联规则，首先从一个频繁项集开始，集合中的元素是不重复的，但我们想知道基于这些元素能否获得其他内容。某个元素或者某个元素集合可能会推导出另一个元素。例如，一个频繁项集｛豆奶, 莴苣｝，可能有一条关联规则“豆奶→莴苣”，这意味着如果有人购买了豆奶，那么在统计上他购买莴苣的概率较大。但是这条反过来并不总是成立。换言之，即使“豆奶→莴苣”统计上显著，那么“莴苣→豆奶”也不一定成立。箭头的左边集合称作前件，箭头右边的集合称为后件。 上节我们给出了繁琐项集的量化定义，即它满足最小支持度要求。对于关联规则，我们也有类似量化方法，这种量化标准称为可信度。一条规则P→H的可信度定义为support(P | H) / support(P)。在前面我们已经计算了所有繁琐项集支持度，要想获得可信度，只需要再做一次除法运算。 从一个繁琐项集中可以产生多少条关联规则？下图给出了从项集{0,1,2,3}产生的所有关联规则。为了找到感兴趣的规则，我们先生成一个可能的规则列表，然后测试每条规则可信度。如果可信度不满足最小要求，则去掉该规则。 可以观察到，如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求。具体而言，如果012→3是一条低可信度规则，则所有其它3为后件的规则都是低可信度。这需要从可信度的概念去理解，Confidence(012→3) = P(3|0,1,2), Confidence(01→23)=P(2,3|0,1)，P(3|0,1,2) &gt;= P(2,3|0,1)。由此可以对关联规则做剪枝处理。 还是以之前的杂货店交易数据为例，我们发现了以下频繁项集： 对于寻找关联规则来说，频繁1项集L1没有用处，因为L1中的每个集合仅有一个数据项，至少有两个数据项才能生成A→B这样的关联规则。取置信度为0.7，最终从L2发掘出10条关联规则： 接下来是L3： 假设有一个L4项集（文中的数据不能生成L4），其挖掘过程如下： 利用此性质来减少测试的规则数目，可以先从一个频繁项集开始，接着创建一个规则列表，其中规则右部只包含一个元素，然后对这些规则测试。接下来合并所有剩余规则来创建一个新的规则列表，其中右部包含两个元素。这种方法称为分级法。打开apriori.py，加入如下代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 关联规则生成函数，此函数调用其他两个函数rulesFromConseq、calcConf# L: 频繁项集列表# supportData: 包含那些频繁项集支持数据的字典# minConf: 最小可信度阈值，默认是0.7# 函数最后要生成一个包含可信度的规则列表，后面可以基于可信度对它们进行排序# 这些规则存放在bigRuleList中。def generateRules(L, supportData, minConf=0.7) : bigRuleList = [] # 遍历L中的每一个频繁项集并对每个频繁项集创建只包含单个元素集合的列表H1， # 因为无法从单元素项集中构建关联规则，所以要从包含两个或者更多元素的项集开始规则构建过程。 # 只获取有两个或更多元素的集合 for i in range(1, len(L)) : for freqSet in L[i] : H1 = [frozenset([item]) for item in freqSet] if i &gt; 1 : # 如果频繁项集的元素数目超过2，那么会考虑对它做进一步的合并，合并通过 # rulesFromConseq来完成 rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else : # 如果项集中只有两个元素，那么需要使用calcConf()来计算可信度值 calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleList# 对规则进行评估# 目标是计算规则的可信度以及找到满足最小可信度要求的规则# 函数会返回一个满足最小可信度要求的规则列表，空列表prunedH保存这些规则def calcConf(freqSet, H, supportData, brl, minConf=0.7) : prunedH = [] # 遍历H中的所有项集并计算它们的可信度值 for conseq in H : # 可信度计算时使用supportData中的支持度数据 conf = supportData[freqSet] / supportData[freqSet - conseq] # 规则满足最小可信度值，将这些规则输出到屏幕显示 if conf &gt;= minConf : print(freqSet-conseq, '--&gt;', conseq, 'conf:', conf) brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedH# 用于生成候选规则集合，从最初的项集中生成更多的关联规则# freqSet: 频繁项集# H: 可以出现在规则右部的元素列表def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7) : # H中频繁项集大小m m = len(H[0]) # 查看该频繁项集是否大到可以移除大小为m的子集 if (len(freqSet) &gt; (m+1)) : # 生成H中元素的无重复组合，结果存储在Hmp1，这也是下一次迭代的H列表 Hmp1 = aprioriGen(H, m+1) # Hmp1包含所有可能的规则，利用calcConf()来测试它们的可信度以确定是否满足要求 Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf) # 如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq if (len(Hmp1) &gt; 1) : rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf) 检验运行效果： 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; import apriori&gt;&gt;&gt; dataSet = apriori.loadDataSet()&gt;&gt;&gt; dataSet[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]# 生成一个最小支持度为0.5的频繁项集的集合&gt;&gt;&gt; L, supportData = apriori.apriori(dataSet, minSupport=0.5)&gt;&gt;&gt; rules = apriori.generateRules(L, supportData, minConf=0.7)frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0frozenset([1]) --&gt; frozenset([3]) conf: 1.0&gt;&gt;&gt; rules[(frozenset([5]), frozenset([2]), 1.0), (frozenset([2]), frozenset([5]), 1.0), (frozenset([1]), frozenset([3]), 1.0)]# 得到了3条规则：&#123;5&#125;→&#123;2&#125;、&#123;2&#125;→&#123;5&#125;、&#123;1&#125;→&#123;3&#125;，可见包含2和5的规则可以互换前后件，包含1和3的不行# 接下来降低可信度阈值，可以得到结果如下&gt;&gt;&gt; rules = apriori.generateRules(L, supportData, minConf=0.5)frozenset([3]) --&gt; frozenset([2]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([5]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0frozenset([3]) --&gt; frozenset([1]) conf: 0.666666666667frozenset([1]) --&gt; frozenset([3]) conf: 1.0frozenset([5]) --&gt; frozenset([2, 3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([2, 5]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3, 5]) conf: 0.666666666667&gt;&gt;&gt; rules[(frozenset(&#123;3&#125;), frozenset(&#123;2&#125;), 0.6666666666666666), (frozenset(&#123;2&#125;), frozenset(&#123;3&#125;), 0.6666666666666666), (frozenset(&#123;5&#125;), frozenset(&#123;3&#125;), 0.6666666666666666), (frozenset(&#123;3&#125;), frozenset(&#123;5&#125;), 0.6666666666666666), (frozenset(&#123;5&#125;), frozenset(&#123;2&#125;), 1.0), (frozenset(&#123;2&#125;), frozenset(&#123;5&#125;), 1.0), (frozenset(&#123;3&#125;), frozenset(&#123;1&#125;), 0.6666666666666666), (frozenset(&#123;1&#125;), frozenset(&#123;3&#125;), 1.0), (frozenset(&#123;5&#125;), frozenset(&#123;2, 3&#125;), 0.6666666666666666), (frozenset(&#123;3&#125;), frozenset(&#123;2, 5&#125;), 0.6666666666666666), (frozenset(&#123;2&#125;), frozenset(&#123;3, 5&#125;), 0.6666666666666666)]# 一旦降低可信度阈值，就可以获得更多的规则 Apriori应用 之前我们在小数据上应用了apriori算法，接下来要在更大的真实数据集上测试效果。那么可以使用什么样的数据呢？比如：购物篮分析，搜索引擎的查询词，国会投票，毒蘑菇的相似特征提取等； 示例：发现毒蘑菇的相似特征 从此处下载mushroom.dat，其前几行如下： 12345671 3 9 13 23 25 34 36 38 40 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 14 23 26 34 36 39 40 52 55 59 63 67 76 85 86 90 93 99 108 114 2 4 9 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 108 115 1 3 10 15 23 25 34 36 38 41 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 16 24 28 34 37 39 40 53 54 59 63 67 76 85 86 90 94 99 109 114 2 3 10 14 23 26 34 36 39 41 52 55 59 63 67 76 85 86 90 93 98 108 114 2 4 9 15 23 26 34 36 39 42 52 55 59 63 67 76 85 86 90 93 98 108 115 第一个特征表示有毒或者可食用，有毒为2，无毒为1。下一个特征是蘑菇伞的形状，有六种可能的值，分别用整数3-8来表示。 为了找到毒蘑菇中存在的公共特征，可以运行Apriori算法来寻找包含特征值为2的频繁项集。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt;&gt;&gt; import apriori&gt;&gt;&gt; mushDatSet = [line.split() for line in open('mushroom.dat').readlines()]# 在数据集上运行Apriori算法&gt;&gt;&gt; L, suppData = apriori.apriori(mushDatSet, minSupport=0.3)# 在结果中可以搜索包含有独特征2的频繁项集：&gt;&gt;&gt; for item in L[1] :... if item.intersection('2'): print(item)...frozenset(&#123;'28', '2'&#125;)frozenset(&#123;'2', '53'&#125;)frozenset(&#123;'2', '23'&#125;)frozenset(&#123;'2', '34'&#125;)frozenset(&#123;'2', '36'&#125;)frozenset(&#123;'59', '2'&#125;)frozenset(&#123;'63', '2'&#125;)frozenset(&#123;'67', '2'&#125;)frozenset(&#123;'2', '76'&#125;)frozenset(&#123;'2', '85'&#125;)frozenset(&#123;'2', '86'&#125;)frozenset(&#123;'2', '90'&#125;)frozenset(&#123;'93', '2'&#125;)frozenset(&#123;'2', '39'&#125;)# 对更大项集来重复上述过程&gt;&gt;&gt; for item in L[3] :... if item.intersection('2') : print(item)...frozenset(&#123;'2', '28', '59', '34'&#125;)frozenset(&#123;'2', '28', '59', '85'&#125;)frozenset(&#123;'2', '86', '28', '59'&#125;)frozenset(&#123;'2', '28', '59', '90'&#125;)frozenset(&#123;'2', '28', '59', '39'&#125;)frozenset(&#123;'63', '2', '28', '39'&#125;)frozenset(&#123;'63', '2', '28', '34'&#125;)frozenset(&#123;'63', '2', '28', '59'&#125;)frozenset(&#123;'63', '2', '28', '85'&#125;)frozenset(&#123;'63', '2', '86', '28'&#125;)frozenset(&#123;'2', '28', '85', '34'&#125;)frozenset(&#123;'2', '86', '28', '34'&#125;)frozenset(&#123;'2', '86', '28', '85'&#125;)frozenset(&#123;'34', '2', '28', '90'&#125;)frozenset(&#123;'2', '28', '85', '90'&#125;)frozenset(&#123;'2', '86', '28', '90'&#125;)frozenset(&#123;'2', '28', '34', '39'&#125;)frozenset(&#123;'2', '28', '85', '39'&#125;)frozenset(&#123;'2', '86', '28', '39'&#125;)frozenset(&#123;'2', '28', '90', '39'&#125;)frozenset(&#123;'34', '2', '28', '53'&#125;)frozenset(&#123;'2', '28', '85', '53'&#125;)frozenset(&#123;'2', '86', '28', '53'&#125;)frozenset(&#123;'90', '2', '28', '53'&#125;)frozenset(&#123;'2', '28', '53', '39'&#125;)...... 接下来你需要观察这些特征，以便知道蘑菇的各个方面，如果看到其中任何一个特征，那么这些蘑菇就很有可能有毒。 小结 关联特征是用于发现大数据集元素间有趣关系的一个工具集，可以采用两种方法来量化这些有趣关系。第一种方法是使用频繁项集，它会给出经常在一起出现的元素项。第二种方式是关联规则，每条关联规则意味着元素之前的“如果……那么”关系。 发现元素间不同的组合是个非常耗时的任务，不可避免需要大量昂贵的计算资源，这就需要一些更智能的方法在合适的时间范围内找到频繁项集。其中一个方法是Apriori算法，它使用Apriori原理减少在数据库上进行检查的集合的数目。Apriori原理是说如果一个元素项是不频繁的，那么那些包含该元素的超集也是不频繁的。Apriori算法从单元项集开始，通过组合满足最小支持度要求的项集来形成更大的集合。支持度用来度量一个集合在原始数据中出现的频率。 关联分析可以用在许多不同物品上。商店中的商品以及网站的访问页面是其中比较常见的例子。关联分析也曾用于查看选举人及法官的投票历史。 缺点是每次增加频繁项集的大小，Apriori算法都会重新扫描整个数据集，当数据集很大时，这会显著降低频繁项集发现的速度。 参考资料：《机器学习实战》、《数据挖掘导论》]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>Association Analysis</tag>
        <tag>Python</tag>
        <tag>Apriori</tag>
      </tags>
  </entry>
</search>
