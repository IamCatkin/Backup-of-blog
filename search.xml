<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[尼氏染色操作流程]]></title>
    <url>%2F2019%2F10%2F25%2F%E5%B0%BC%E6%B0%8F%E6%9F%93%E8%89%B2%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[理由同上。 原理 尼氏染色法(Nissl Staining)是用碱性染料染神经组织的一种方法。尼氏体是胞质内的一种嗜碱性物质，广泛见于各种神经元，不同神经元中的尼氏体形状、大小和数量则各有差异。用于尼氏染色的碱性染料主要有焦油紫、亚甲蓝、甲苯胺蓝和硫堇等。尼氏染色法可以染出尼氏体，用来观察神经元内的细胞结构；还可以通过尼氏染色后对尼氏体的观察来了解神经元的损伤情况 。 步骤 不同老师有方法略有区别，我们以熊鲲老师的方法为例。 将处理好的样本放入尼氏染色液，时间可根据切片厚度等，结合染色情况调整，一般在10分钟-30分钟左右。 蒸馏水洗涤数次，每次几秒。 用无水乙醇分色，每次浸入几秒拿出观察脱色情况，到变蓝或者浅紫色为止。 轻磕载玻片，将水甩干，注意避免组织被甩掉，时间允许则风干。 用无水乙醇脱水3分钟。 （水洗，并甩干）。 二甲苯脱脂3-5分钟。 换新鲜二甲苯再脱脂3-5分钟。 封片：用棉签屁股挑去少许中性树脂到盖玻片上，手持载玻片压下去，注意避免气泡产生。如果产生，可以将气泡挤出。 观察染色情况。]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>Neurons</tag>
        <tag>Nissl body</tag>
        <tag>Staining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冰冻切片机操作流程]]></title>
    <url>%2F2019%2F10%2F22%2F%E5%86%B0%E5%86%BB%E5%88%87%E7%89%87%E6%9C%BA%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[由于要交一份电子版，所以修改一下顺便传上来了。 目的 为什么要做冰冻切片而不是石蜡切片，因为冰冻切片相对于石蜡切片相对简便快速，用时短， 多应用于手术中的快速病理诊断 ，缺点是不利于持久保存。 步骤开机 使用前最少2小时，按右侧电源键开机。但在实际情况下，一般为待机状态，长按待机键解除待机状态。 预冷 调节温度至-25℃，将刀片刷子样品等提前放进去预冷。等待仪器降至所需温度。 固定刀片及样品 在样本托上滴点蒸馏水，用来固定样品，如有需要可用蓝色速冻台降温，冷冻后将其固定在切片机样本头上。一般不需要动刀座的锁紧杆，只需要打开刀片的锁紧杆放入刀片。注意使用仪器左侧控制面板上的样本后退按键，将样本头调到最后方，防止安装样本时撞坏防卷板、刀片等 调节切片厚度 一般调节切片厚度为15或20μm，小心控制仪器的样本推进按键，使样本向刀架方向前进直到刀片快要接触样本时，此时再换用手轮控制。 切片 打开手轮柄锁，顺时针均力旋转手轮一圈，切好后将手轮锁定在12点位置，即样本头移动在最高位置以便于取出切片，掀开防卷板，用常温的针管弯头将切片粘附放入常温放置的蒸馏水或者PBS中，再用带毛针管头小心贴片。当然也有用常温载玻片往样本上靠的方法。如果刀上或防卷板粘有脏东西，用预冷过的刷子顺着刀刃方向刷干净。 清理 切片结束后收好刀片，锁定手轮，用冰冻台里面的刷子扫除多余组织碎片并清理出去，较小组织可以用常温放置的卫生纸擦拭。注意不能用水擦拭，应当用无水乙醇。 关机 一般情况下因为要经常使用，可不必关闭电源。用完将温度调到-10℃，然后长按待机键使时间中间的：消失，即为进入待机状态。 其他注意事项 环境温度应该在20℃下进行。 防卷板轻拿轻放，避免损坏。 切片完毕收好刀片，锁死手轮锁。 其他已设参数勿动，一般只调整切片厚度。 应保持切片机及周围区域清洁，机外废水壶满应及时倒掉。]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>Histopathology</tag>
        <tag>Cryostat</tag>
        <tag>Leica</tag>
        <tag>CM1860</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端学习笔记]]></title>
    <url>%2F2019%2F10%2F19%2F%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[平生不学WEB，便称编程也枉然 ！ HTML5 HTML（HyperText Markup Language）是一种标记语言以便于浏览器正确展示网页给用户。HTML使用标签（tags）来区分用户所能看到的内容和便于浏览器解释的指示。 基础]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HTML5</tag>
        <tag>CSS3</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可怜风絮落溪津]]></title>
    <url>%2F2019%2F10%2F16%2F%E5%8F%AF%E6%80%9C%E9%A3%8E%E7%B5%AE%E8%90%BD%E6%BA%AA%E6%B4%A5%2F</url>
    <content type="text"><![CDATA[木落吴江矣，正萧条、西风南雁，碧云千里。落魄江湖还载酒，一种悲凉滋味。重回首、莫弹酸泪。不是天公教弃置，是南华、误却方城尉。飘泊处，谁相慰。别来我亦伤孤寄。更那堪、冰霜摧折，壮怀都废。天远难穷劳望眼，欲上高楼还已。君莫恨、埋愁无地。秋雨秋花关塞冷，且殷勤、好作加餐计。人岂得，长无谓。 ——纳兰性德《金缕曲·寄梁汾》 如果要我从历史长河里面选一位最喜欢的词人，纳兰性德肯定身处一个尴尬的位置。年少的时候被纳兰的“人生若只如初见”、“当时只道是寻常”吸引，那段时间的作文频频引用他的词句，连QQ签名都改成”我是人间惆怅客“了。后来读的词多了，对纳兰词的喜爱渐渐减了，按叶嘉莹先生所说读纳兰词的三个阶段，怕是我已经到了第二个阶段——见山不是山，见水不是水的境界，以致逐渐无感了。 不过客观的说，纳兰的小令在中国词史上还是有一席之地的。这两天长沙天气转凉，读到这首金缕曲时，也有几分感同身受。纳兰性德尚且同顾贞观互为知己，被世人传为佳话。转念一想，我还挺羡慕他的，毕竟我自己心中郁结之气都不知道能和谁诉说，只能在这胡言乱语了。 前段时间有同学问我做什么兼职能赚钱，我本想说运营自媒体，君不见 Ayawawa 、咪蒙、Hugo等靠一篇篇朋友圈爆款赚的盆满钵满，本以为我对这些贩卖焦虑的自媒体已经免疫了，现在看来我也不知不觉着了道。 说到焦虑，从小到大我一直觉得这个词和我没什么关系。幸运的是，每个阶段都有老师对我青眼有加，让我享受”别人家的孩子“的待遇。当然我也不是全无对手，毕竟还有《哈佛女孩刘亦婷》。于是我顺风顺水，没怎么费力就走到了今天。 于是现在就遭了报应，我厌倦了父母一遍遍将我和别人比较，厌倦了无穷无尽的数落，也厌倦了逼婚。我开始渴望离开家的牢笼，哪怕是进入另一个牢笼。 进组一个多月，也算是经历了不少，参加了一个国际会议的筹办，也打破了我对某些事物的幻想。我是从什么时候开始变得焦虑的呢？是看到同龄人一步步走上正轨，而我却陷入温水煮青蛙的境地的时候吗？说来也可笑，在我想振作的地方，却被弃之如敝屣；另一头又想不断压榨我的剩余价值，三个月不长也不短，足以把我榨干。 当然我也不是水果，榨干我没有任何好处，对我自己也是。我安慰自己，这是我的兴趣，在这样的阿Q精神的指引下继续前进。其实这两年也并不是毫无收获，比如现实彻底把我从转行IT的美梦给砸醒，比如IntelliJ IDEA到底是卸载还是不卸载的困扰，再比如今晚的呢喃呓语让我忘掉了明天依然还要早起签到的痛苦。 之前和师姐说到课题的事，她说你研一就好好玩，反正也不会有课题的，不要每天担心不能毕业的事。我问那研二就有课题了吗，她说研二你就会接受延毕的命运了。呵，真操蛋。 说不出现在是后悔还是不后悔，两年以前，当我还是一位保险人时，恐怕也没想到今天我还会重返课堂，就跟我一年前也没想到我的研究生之路会如此曲折一样，这就是生活，永远会往我想不到的方向前进。 话又说回来，人还是得活的洒脱一点，就算落魄江湖，至少我还有酒喝。]]></content>
      <categories>
        <category>负尽年华</category>
      </categories>
      <tags>
        <tag>Daily</tag>
        <tag>Feeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[悬疑推理向游戏记录]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%82%AC%E7%96%91%E6%8E%A8%E7%90%86%E5%90%91%E6%B8%B8%E6%88%8F%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[记录一下玩过的悬疑推理向游戏，因为显得比较不务正业，所以决定藏在下面一点，排名按照个人喜好，分先后。 428:被封锁的涩谷 严格意义上讲，这个不能算推理游戏，应该归为悬疑游戏。不过叙事上非常优秀，而且结局众多，众多情节都很合乎逻辑，相互照应。是Fami通唯一一个满分的音像小说。 这个游戏以涉谷一天中5个不同主角的不同行为来推演结果，其中一个主角无厘头的行为有可能就决定了另外一个主角的生死，当最后所有人的剧本交织在一起的时候，那种美妙的感觉实在是让人感叹。 本作是我最喜欢的ADV，个人评分：9.6分。 Ever 17 meta类的游戏，它并不是第一个，诡叙类的游戏，它也不是第一个，不过把这些核心诡计藏到最后，直到最后一刻才揭晓，难怪大部分玩过这部作品都要称为“神作”。作为一部02年的作品，它的地位大概可以类比《占星术杀人魔法》在推理小说中的地位，属于开创性的。 由于我不喜欢Galgame中冗长的日常，所以觉得前面四条线日常有点偏多，而这游戏属于必须通完五条线才能看到真结局的，觉得有点拖沓。但是通关后会有豁然开朗之感，之前的一些伏线基本上全部收回。只能佩服打越钢太郎的布局和脑洞。 总结而言，本作是打越钢太郎的巅峰之作，他后面的作品都未能再有突破，个人评分：9.5分。 大逆转裁判系列 合并在一块是因为这个作品单看第一部并不够出色，很多线没收，只有一二部合在一起才算构成了闭环。逆转裁判系列在我心目中尚且不如这部外传。 整个作品设定在十九世纪末二十世纪初的英国，音乐和画风确实很还原，用了不少细节刻画当时的风土人情，而且我喜欢的福尔摩斯也在里面出场，就推理质量而言，整体案件二比一强了不少，而且二基本上把一里面挖的坑填完了，硬生生挽回了大逆转裁判一几乎崩掉的口碑。 个人而言更喜欢这个系列，逆转裁判本作更类似于设定系推理，我是不太喜欢推理作品里面有超自然因素的，相对而言大逆转裁判更加古典一点。 个人评分：大逆转裁判一8分，大逆转裁判二9.2分，合起来可以算9.1分。 命运石之门 这部作品比Ever 17强在动画化了，而且人设更加讨喜，因此名气大了不少。不过就我个人而言，一方面觉得这种LOOP类的游戏已经珠玉在前，创新性扣分，此外日常部分就更多，玩起来就更觉得拖沓了。 但是这部游戏作为科学幻想系列作品，世界观的设定更为严谨一点，谜面解答也很公平，称得上是佳作。 个人评分：9.0分。 弹丸论破系列 这个游戏系列的名气还是很高的，毕竟也有过动画化。个人而言比较喜欢这个系列的题材，暴风雪山庄的设定加上水平尚可的案件，量大管饱，而且比逆转裁判系列代入感和推理性要更强一点，逆转裁判的问题稍后再说。 案件质量而言应该是2&gt;1&gt;V3，V3的问题一方面人设感觉没之前有亮点，另一方面后期有点放飞自我，直接变meta了，感觉把系列的路全堵死了，不过中间玩的Trick还算有点意思，扣住了主题“谎言”。 总体来说还是相当推荐的，毕竟推理性很强，比起《逆转裁判》这类的更本格一点。在《诡计×逻辑》的汉化出来之前（恐怕也不太可能出来）似乎也就这个更符合我的审美了。 个人评分：弹丸论破1为8.8分，弹丸论破2为9.0分，弹丸论破V3为8.6分，综合来看系列分大概在8.9左右。 极限脱出系列 打越钢太郎的代表作之一了，第一部《极限脱出:9小时9个人9之门》是该系列的巅峰之作，最终的Trick配合NDS特有的双屏给人的震撼无以复加。当然核心诡计抄了打越自己的《Ever 17》，这就是传说中的我抄我自己。 从第二部《极限脱出:善人死亡》开始，就开始一部比一部扑，诡计上也一部不如一部，第三部还是在欧美粉丝的支持下才出来的，不过建模岂是一个崩字了得，可见经费捉襟见肘。估计也不会有续作了，还是挺遗憾的。不过要说到密室脱逃，个人感觉手机上爆火的《Room》远不如这个。 个人评分：极限脱出1为9.2分，极限脱出2为8.8分，极限脱出3为8.4分，综合来看系列分估计也就给个8.8左右。 逆转裁判系列 这里把逆转裁判1-6和逆转检事1-2以及雷顿教授vs逆转裁判拉一块说。 逆转裁判的切入点确实很好，在那个年代的游戏基本上都是侦探视角的破案游戏，巧舟把观念逆转过来，弄了个法庭辩论游戏，当然法庭辩论在侦探小说界也算不上新梗，毕竟卡尔的名作《犹大之窗》早就涉及相关题材，但是这个游戏也算是开辟了推理ADV的新道路。 个人觉得在逆转裁判1-3和逆转检事1-2之后这个系列已经露出了疲态，该写的已经都写了，按理说应该可以完结了。卡普空为了继续捞钱，强行开续作，而之后的增加的主角王泥喜和心音完全撑不起逆转裁判这个IP，案件乏善可陈，玩下去已经成为一种惯性，我猜大概也是巧舟要重新开大逆转裁判系列的目的吧。 《雷顿教授vs逆转裁判》这一作联动完全对不住两个IP的名头，剧情也有点扯淡，亮点恐怕就是有配音的逆转裁判了。 个人评分：逆转裁判三部曲8.7分，逆转检事两部曲8.8分，剩下的4-6大致在8.4-8.6分左右，雷逆8.0分，综合评价8.7分。 AI：梦境档案 打越新作，这次国庆肝了3天全通了。不得不说，现在的推理悬疑向游戏太少了，连一年一部都很难保证。 本来是期待一部超越《Ever 17》或者《极限脱出》的作品，不过看上去打越对市场妥协了，做了一部相对比较讨喜市场的作品。游戏里有非常多玩梗的地方，看到会给人会心一笑的感觉，相对于他之前的作品，整体氛围也轻松了不少。不同支线埋的坑，基本上都在最后回收了，还是可以见到打越的功力的。然而这部作品的谜面并不出众，基本上玩到中途就能猜到结尾了，给我的惊喜不够。 另外本作的Psync感觉没《极限脱出》系列的密室逃脱有意思，大概还是对市场的妥协吧，毕竟不能再承受一次《善人死亡》的暴死了吧。 更新：听说这次在日本销量又暴死了，这次可能是里面黄段子太多了吧哈哈哈。 个人评分：8.5分。 G弦上的魔王 同样是诡叙性的作品，而且游戏一直在刻意误导玩家，然而最后的反转会让人感觉铺垫不足，不够严谨。优点在于是斗智类剧情，能够吸引人一直玩下去，BGM也是其中的亮点，基本上全部是世界古典名曲或变奏。 总的来说，是一部主线强而支线较弱的作品，值得一口气玩下去，但是核心诡计不够完美应该扣分。 个人评分：8.0分。]]></content>
      <categories>
        <category>缓寻芳草</category>
      </categories>
      <tags>
        <tag>Game</tag>
        <tag>Visual novel</tag>
        <tag>Lattice resoning</tag>
        <tag>Galgame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推理小说记录]]></title>
    <url>%2F2019%2F09%2F05%2F%E6%8E%A8%E7%90%86%E5%B0%8F%E8%AF%B4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[谨以此文记录看过的推理小说。 日本东野圭吾 名气实在太大了，感觉没有必要靠俺推荐，知名作品就不写了，写点小众一点的。 《假面山庄》 这部小说最大的问题就是看起来是暴风雪山庄，结果最后的结果又全推翻了，看起来想打人。 岛田庄司 作为新本格的代表人物，岛田庄司的作品很多，有名的有：《占星术杀人魔法》、《斜屋犯罪》、《奇想天动》、《异邦骑士》、《北方夕鹤2/3杀人事件》、《螺丝人》，惭愧的是大部分我都看不下去，所以就提2部。 《占星术杀人魔法》 很遗憾，读到这篇小说的时候，我已经看过金田一和少年包青天的类似案件了，知道了核心诡计，小说的魅力大减，不过作为首个用出该核心诡计的小说，被抄袭不是它的错，只能说这个诡计设计太精妙了。 《斜屋犯罪》 很有意思的密室杀人，宏大的场面，就诡计而言确实瞒过我了，而且那个建筑物有点复杂，看的时候来来回回研究了半天。当然动机觉得有点弱，总的来说值得一读。 绫辻行人 绫辻行人的主要作品就是馆系列，值得一提的是其中的一位侦探主角的名字岛田洁就是来源于岛田庄司和他笔下的侦探御手洗洁。馆系列的作品质量良莠不齐，也就如下几部值得一看。 《钟表馆事件》 公认的馆系列最高作，双线叙事，暴风雪山庄模式+ 时间诡计，加上作者的叙述性诡计，整体看下来是相当精彩的，不过得对照地图来看。 《十角馆事件》 感觉是日本版的《无人生还》，当然没有童谣杀人的设定，由于是伪暴风雪山庄，主要是看凶手制造不在场证明的手法，配合作者的诡叙，还是值得一看的，动机设定就emmm了。 《迷宫馆事件》 这个系列看下来，就是觉得作者为了一个故事就设定一个建筑确实很敬业。就这本书而已，还是惯用的诡叙，事件本身很吸引人，但解答看起来有种被愚弄的感觉。 森博嗣 森博嗣是因为之前看过小说同名电视剧《全部成为F》，毕竟单元推理剧并不是很多，看完后对他的小说有点兴趣，又翻出来看。不过他的小说还是很有特点的，有种特有的理工科的浪漫。 《全部成为F》 应该是在大陆名气最高的作品了，不过也有可能是其他作品不在大陆出版的缘故。当然小说内容本身就在电视剧里被改编过了，得知了谜底的情况下，也就只剩下对比细节了。 《不会笑的数学家》 看过的感觉就是《三星馆事件》？和斜屋犯罪或者馆系列一样的建筑推理，核心诡计有点意思，当然这类推理都这样，反正森博嗣的S&amp;M系列看的是教授和学生的恋爱日常23333。 《死亡幻术的门徒》 听说这部在日本评价很高，看完后感觉一般，总共有3起案件，谜设看起来挺华丽的，不过解谜感觉都挺一般的，反正我觉得有点扯淡，感觉这种诡计侮辱我智商。 今村昌弘 他的《尸人庄谜案》之前就久闻大名，等了很久才有中文版上市，只想说：“磨铁出来挨打。” 《尸人庄谜案》 期待了2年的作品，读完觉得有点捧过头了，但是还是挺有意思的。在现代信息发达的社会很难设计出一个合理的暴风雪山庄，这本书的给了一个合乎逻辑的设定。案件设计上还是合乎情理，算得上自圆其说，但是有隐瞒线索的嫌疑。不过读完只能感叹现在推理界不景气啊，这本书并不差，但是对不起它独霸三榜的地位。 《魔眼之匣谜案》 个人觉得不如《尸人庄》，也不是那种很本格的推理，密室与杀人亮点不够，但是整体的结构挺精巧，最后的反转还是有点冲击力的。 西泽保彦 最出名的大概是《解体诸因》了，剩下几本还没读，读了再更新吧。 《解体诸因》 短篇合集，作者一本正经的对分尸的理由做了一系列解答，读起来还是挺有意思的。主要是都是中短篇，看起来也不累。 《死了七次的男人》 作为SF推理，创意上有点意思，可惜没什么推理，剧情槽点满满。 青崎有吾 话说作者也就比俺大两岁，21岁就凭《体育馆之谜》拿奖出道，俺深感惭愧啊。 《体育馆之谜》 这部的诡计被《唐人街探案》抄袭了，虽然挺本格，但是我不太喜欢宅男侦探的人设，和作者偏轻小说的文风。所以也仅仅看了这部。 《敲响密室之门》 哎呀被打脸了，最近补了这本，果然推理还是看中短篇比较轻松。 《水族馆之谜》 逻辑流，真的废话好多，不知道怎么被我看完的。 东川笃哉 看了以他原著改编的电视剧《推理要在晚餐后》才关注的，总觉得文风太轻小说化了，甚至到了感觉有点尬的地步。 《请勿在此丢弃尸体》 实在是太轻小说了，各种角色跟白痴一样，谜设一般般，觉得应该适合于中学生来看，如果能get到他设的各种笑点，阅读体验大概还行吧。 《密室的钥匙借给你》 严格意义上讲，比上面那本读起来更舒服一点，不过动机和手法就不咋地了，而且突然就哲学了起来emmmm。 《推理要在晚餐后》 跳过了被改编成剧的部分，严格意义上讲确实质量不高，不过阅读体验还可以，比起有些作家的晦涩的文字，至少他的文字还算可读。 石持浅海 作者好像喜欢用倒叙推理，《紧闭的门扉》便是如此，我也只看了这本。 《紧闭的门扉》 大概是喜欢《古畑任三郎》，所以对倒叙推理还是挺有兴趣的，这种类型的推理不怕泄底，更看重侦探和凶手的对决，唯一的问题在于，凶手好像有点弱，侦探的屁股也很歪。 早坂吝 传说中的工口推理作家，估计也不太可能在国内出版，不过《彩虹牙刷》有民翻。 《彩虹牙刷》 工口推理赛高，看起来好像是小黄文，不过里面的推理还是挺正经的。结局的脑洞大开，值得一看。 深水黎一郎 只看过《推理竞技场》，个人感觉作者的文风不错。 《推理竞技场》 很娱乐的推理小说，作者一直在戏谑诡叙和多重解答，有种批评文学的感觉。就推理性而言不值一提，当故事来看尚可一读。 泡坂妻夫 这个作者的小说布局很有意思，属于那种炫技流，把其他领域的知识引入到推理小说，又不觉得卖弄，确实有真才实学的那种。 《十一张牌》 作者之前好像是魔术师，所以这本小说对魔术如数家珍，堪称魔术泄底大全。读起来也挺有意思的。 大山诚一郎 是近年来水平比较高的作家了，《赤色博物馆》、《密室收藏家》评价口碑都不错，希望新书能早日看到翻译。 《密室收藏家》 总共有5篇很纯粹的密室故事，时间跨度60年，贯穿其中的就是自称密室收集家的一位，容貌永远不老，只要有密室出现就仿若从天而降一般的来解谜的神秘人物。整体质量不错，很正统的本格推理，强烈推荐。 《赤色博物馆》 依旧是短篇集，不过每篇故事都算很完整，写的是从未解决事件的证物细节中发掘真相的警察，读起来感觉还行，感觉不如上一本《密室收藏家》，也算值得一读了。 市川忧人 代表作品《水母不会冻结》和《蓝玫瑰不会安眠》，作为新锐推理小说作家，对他期待挺高，不过读过《水母不会冻结》后，感觉有点失望，不由对近年来另一部名气很大的推理作品《尸人庄杀人事件》感到忧心啊。 《水母不会冻结》 特点是暴风雪山庄和SF，号称致敬《无人生还》。不过也仅仅是个噱头，可能是叙述手法的问题，揭秘时没什么感觉，诡计也觉得一般。 《蓝玫瑰不会安眠》 和上一部一样的双线叙事的写作手法，配合叙诡，确实没想到是这样的一个布局，但是诡计还是感觉有点扯淡。总的来说可以一看，个人觉得比水母那本强点。 歌野晶午 此人我只看过一部小说，已经隐隐有拉黑趋势了。 《樱树抽芽时，想你》 这是我看过的此人第一部小说。平心而论，就行文方式，确实读着很舒服，然而最核心的诡叙却让我很不信服，虽然解释了之前的一些让我疑惑的点，但是有些地方却让我更加觉得很雷人，甚至有点恶心。 白井智之 鬼畜推理代表之一，读完已被拉黑。 《杀死少女的100种方法》 五个案子，除了第三个案子有点黑色幽默的感觉，其他几个读起来只觉得恶心，感觉作者是心理变态。虽然是很本格的推理，但是我不喜欢他的内容。 中西智明 这老哥凭一部《消失！》出道，然后就没声音了。 《消失！》 八嘎推理作品，诡叙核心我没全猜到，只能说这种作品放在三十年前应该是惊艳的作品，可惜俺出生太晚，读到的时候也太晚了。 中国孙沁文 笔名鸡丁，国产推理作家。只看过《凛冬之棺》。 《凛冬之棺》 包括3个密室，整体来说可以自圆其说，设计上还是挺有想象力的，不过总感觉和一些优秀的国外推理小说比差点味的样子。 陈浩基 香港推理作家，很早就听说过《13 67》的名声了，最近才得以拜读。 《13 67》 6篇故事，以主人公关振铎的视角，描述了香港的一个时代的变迁。就单篇而言是本格推理，放一块又有种社会派的感觉。最后一篇与第一篇遥相呼应，结构上给人很精妙的感觉。推荐一读。听说要改编电影？希望别毁了。 时晨 号称本格良心，逻辑良心，俺看完《黑曜馆事件》后就起了嘀咕，《镜狱岛事件》看了一点就弃了，给写出这样评价的人劈个叉吧。 《黑曜馆事件》 姑且评价一下，暴风雪山庄题材，本来俺挺感兴趣的，不过作者的核心手法都有问题，贯穿始终的“暗黑童话”感觉很可笑，这么一看的话，可读性就一般了，对不起豆瓣上那堆吹上天的评价。 杨叛 小时候在《今古传奇·武侠版》里面读过一点他的小说，很后面才补他的云寄桑三部曲。 《死香煞》 武侠+推理，是我喜欢的两个元素，这块写的好的并不多，小时候读的古龙算是其中的佼佼者了，作为武侠，很多在推理作品里面没法使用的诡计也能合理用出，读起来更有新奇感。可惜就这篇而言，虽然用了暴风雪山庄模式，总感觉情节和冲突都缺点味道，不够成熟。 《鬼缠铃》 其实整体上还行，就是老穿插男主的精神病，感觉故弄玄虚，另外那些配角各种设定，让人觉得神叨叨的，结果后面也不过如此。剩下的因为主角设定受了伤，没上一本那样的神功破案，相对来说限制多了点，可能就推理体验更好。 《傀儡咒》 读起来是因为惯性，其实亮点有限，读起来没什么意外感。而且我不太喜欢里面的棒子。 《自在花》 短篇，读的时候大概猜到了反转，不过也不难看。 欧美约翰·狄克森·卡尔 传说中的密室推理之王，暂时只看过《犹大之窗》。 《犹大之窗》 法庭推理，庭审和密室的二合一，有种逆转裁判的感觉。诡计嘛，个人估计一般，不过考虑到成书年代，也就凑合看了。]]></content>
      <categories>
        <category>缓寻芳草</category>
      </categories>
      <tags>
        <tag>Lattice resoning</tag>
        <tag>Detective story</tag>
        <tag>Penetralium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记]]></title>
    <url>%2F2019%2F04%2F12%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近在学习一些WEB相关知识，这篇博文用于记录一些知识点，基于Django2.2来编写。 初始化新建虚拟环境 安装virtualenv： 1pip install virtualenv virtualenv使用： 123456# 创建virtualenv &lt;虚拟环境名称&gt;# 启动activate# 退出deactiavte 一键导出/安装扩展 命令提示符下： 12pip freeze &gt; requirements.txtpip install -r requirements.txt 基础知识新建项目 CMD下输入：django-admin startproject &lt;项目名&gt; 初始化数据库 CMD下输入：python manage.py migrate 启动本地项目 CMD下输入：python manage.py runserver 创建管理员 CMD下输入：python manage.py createsuperuser 创建应用 CMD下输入：python manage.py startapp &lt;应用名&gt; 在app目录下定义models.py，例如： 123class Article(models.Model): title = models.CharField(max_length=30) content = models.TextField() 保存后在项目目录下，settings.py注册该应用： CMD下执行： basic12python manage.py makemigrationspython manage.py migrate 在应用目录下，编辑admin.py： 12from .models import Articleadmin.site.register(Article) 绑定url 在项目目录下，编辑urls.py，参考注释： 12Add an import: from my_app import viewsAdd a URL to urlpatterns: path(&apos;&apos;, views.home, name=&apos;home&apos;) 注意事项 公用全局设置可放在settings中，统一管理 12from django.conf import settingssettings.XXX 进阶知识定制admin后台 在应用目录下，models.py中的类中，增加__str__方法，例如： 123456789from django.db import models# Create your models here.class Article(models.Model): title = models.CharField(max_length=30) content = models.TextField() def __str__(self): return "&lt;Article: %s&gt;" % self.title 项目目录下的admin.py： 1234567891011from django.contrib import adminfrom .models import Article# Register your models here.@admin.register(Article)class ArticleAdmin(admin.ModelAdmin): list_display = ('id','title','content') ordering = ('id',) # 倒序：('-id',)# admin.site.register(Article, ArticleAdmin) 修改模型 模型修改后要重新执行 12python manage.py makemigrationspython manage.py migrate 增加其他参数 修改models.py，增加如下内容： 1234567from django.utils import timezoneclass Article(models.Model):...... created_time = models.DateTimeField(auto_now_add=True) last_updated_time = models.DateTimeField(auto_now=True) author = models.ForeignKey(User, on_delete=models.DO_NOTHING,default=1) 相应的应于admin中的list_display增加字段。 给类增加默认排序，在父类中增加子类，形如： 12345678910111213141516171819202122from django.db import modelsfrom django.contrib.auth.models import Userclass BlogType(models.Model): type_name = models.CharField(max_length=15) def __str__(self): return self.type_nameclass Blog(models.Model): title = models.CharField(max_length=50) blog_type = models.ForeignKey(BlogType,on_delete=models.DO_NOTHING) content = models.TextField() author = models.ForeignKey(User, on_delete=models.DO_NOTHING) created_time = models.DateTimeField(auto_now_add=True) last_updated_time = models.DateTimeField(auto_now=True) def __str__(self): return "&lt;Blog: %s&gt;" % self.title class Meta: ordering = ['-created_time'] 使用Shell命令 命令提示符下输入： 1python manage.py shell 管理博文12345678910111213141516171819202122232425&gt;&gt;&gt;from blog.models import Blog&gt;&gt;&gt;from blog.models import BlogType&gt;&gt;&gt;from django.contrib.auth.models import User# 常用&gt;&gt;&gt;Blog.objects.all()&gt;&gt;&gt;BlogType.objects.all()&gt;&gt;&gt;User.objects.all()# 实例化&gt;&gt;&gt;blog = Blog()&gt;&gt;&gt;blog.title = 'shell下第1篇'&gt;&gt;&gt;blog.content = 'xxxxxxxxxxxxxxx'&gt;&gt;&gt;blog_type = BlogType.objects.all()[0]&gt;&gt;&gt;blog.blog_type = blog_type&gt;&gt;&gt;user = User.objects.all()[0]&gt;&gt;&gt;blog.author = user# 保存&gt;&gt;&gt;blog.save()#批量添加&gt;&gt;&gt;for i in range(1,31):... blog = Blog()... blog.title = "for %s" % i... blog.content = 'xxxx:%s' % i... blog.blog_type = blog_type... blog.author = user... blog.save() 网页搭建使用模版 应用目录下创建文件夹templates，然后创建html文件，编辑view.py，类似以下编辑： 1234567891011from django.shortcuts import renderfrom article.models import Articledef article_detail(request, article_id): try: article = Article.objects.get(id=article_id) context = &#123;&#125; context['article_obj'] = article return render(request, "article_detail.html", context) except Article.DoesNotExist: raise Http404('不存在') 也可以使用render_to_response方法： 123from django.shortcuts import render_to_response...... return render_to_response("article_detail.html", context) 还有get_object_or_404方法： 12345678from django.shortcuts import render_to_response,get_object_or_404from article.models import Articledef article_detail(request, article_id): article = get_object_or_404(Article, pk=article_id) context = &#123;&#125; context['article_obj'] = article return render_to_response("article_detail.html", context) html文件类似： 123456789&lt;html&gt; &lt;head&gt; &lt;body&gt; &lt;h2&gt;&#123;&#123; article_obj.title &#125;&#125;&lt;/h2&gt; &lt;hr&gt; &lt;p&gt;&#123;&#123; article_obj.content &#125;&#125;&lt;/p&gt; &lt;/body&gt; &lt;/head&gt;&lt;/html&gt; 创建目录 创建目录html页面，在urls里绑定，使用循环来遍历文章id，使用pk而不是id是更保险的写法。 123456789&lt;html&gt; &lt;head&gt; &lt;body&gt; &#123;% for article in articles %&#125; &lt;a href="/article/&#123;&#123; article.pk &#125;&#125;"&gt;&#123;&#123; article.title &#125;&#125;&lt;/a&gt; &#123;% endfor %&#125; &lt;/body&gt; &lt;/head&gt;&lt;/html&gt; 超链接部分也可以用这种写法： 1&lt;a href="&#123;% url 'article_detail' article.pk %&#125;"&gt;&#123;&#123; article.title &#125;&#125;&lt;/a&gt; url合并 如果有很多应用，按之前的写法，url会太臃肿，因此可以在应用下新建urls.py： 1234567from django.urls import pathfrom . import viewsurlpatterns = [ path('&lt;int:article_id&gt;', views.article_detail, name='article_detail'), path('', views.article_list, name='article_list'),] 相应的，项目下url可改为： 123456789from django.contrib import adminfrom django.urls import include,pathfrom . import viewsurlpatterns = [ path('admin/', admin.site.urls), path('',views.index), path('article/',include('article.urls')),] 使用html模版 为了减少重复html代码的使用，可以使用block块来标记可替代内容，首先在项目根目录下创建templates文件夹，创建base.html并写入以下内容： 123456789101112131415&lt;!-- &#123;% block &lt;别名&gt;%&#125;&#123;% endblock %&#125; --&gt;&lt;!DOCTYPE html&gt;&lt;html lang="zh-cn"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;a href="&#123;% url 'home' %&#125;"&gt;&lt;h3&gt;个人博客网站&lt;/h3&gt;&lt;/a&gt; &lt;/div&gt; &lt;hr&gt; &#123;% block content %&#125;&#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt; 与之相应地，可以在其他的html文件中，省略这些内容，只需要用extends引用，再到block块中填充即可，例如blog_list.html： 123456789101112131415161718192021&lt;!-- &#123;% extends 'base.html' %&#125; --&gt;&lt;!-- &#123;% block &lt;别名&gt;%&#125;&#123;% endblock %&#125; --&gt;&#123;% extends 'base.html' %&#125;&#123;# 页面标题 #&#125;&#123;% block title %&#125; 我的网站&#123;% endblock %&#125;&#123;# 页面内容 #&#125;&#123;% block content %&#125; &#123;% for blog in blogs %&#125; &lt;a href="&#123;% url 'blog_detail' blog.pk%&#125;"&gt; &lt;h3&gt;&#123;&#123; blog.title &#125;&#125;&lt;/h3&gt; &lt;/a&gt; &lt;p&gt;&#123;&#123; blog.content|truncatechars:30 &#125;&#125;&lt;/p&gt; &#123;% empty %&#125; &lt;p&gt;-- 暂无博客，敬请期待 --&lt;/p&gt; &#123;% endfor %&#125; &lt;p&gt;一共有&#123;&#123; blogs|length&#125;&#125;篇文章&lt;/p&gt;&#123;% endblock %&#125; 然后在项目文件夹中的setting.py中编辑TEMPLATES的DIRS列表，加入os.path.join(BASE_DIR, &#39;templates&#39;)。 使用css 在项目目录下新建static文件夹，写入base.css文件： 123456789101112131415161718* &#123; margin: 0; padding: 0;&#125;div.nav &#123; background-color: #eee; border-bottom: 1px solid #ccc; padding: 10px 5px&#125;div.nav a&#123; text-decoration: none; color:black; padding: 5px 10px;&#125;div.nav a.logo &#123; display: inline-block; font-size: 120%;&#125; 然后在项目文件夹中的setting.py中加入： 123STATICFILES_DIRS = [ os.path.join(BASE_DIR, 'static'),] 引用方法： 1234567&lt;!-- head中写入 --&gt;&lt;link rel="stylesheet" href="/static/base.css"&gt; &lt;!-- 也可以用django自带方法，在html顶部写入 --&gt;&#123;% load staticfiles %&#125;&lt;!-- 再到head中写入 --&gt;&lt;link rel="stylesheet" href="&#123;% static 'base.css'%&#125;"&gt; 使用css框架Bootstrap基础 访问官网，下载新版Bootstrap，保留以下文件，放在项目static的新建的Bootstrap文件夹中： 1234567css bootstrap.min.css bootstrap.min.css.mapfont alljs bootstrap.min.js 参考文档，如果需要导入，可按如下代码： 12345678910111213141516171819&#123;% load staticfiles %&#125;&lt;!DOCTYPE html&gt;&lt;html lang="zh-cn"&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt; &lt;link rel="stylesheet" href="&#123;% static 'base.css' %&#125;"&gt; &lt;link rel="stylesheet" href="&#123;% static 'bootstrap-3.3.7/css/bootstrap.min.css' %&#125;"&gt; &lt;script type="text/javascript" src="&#123;% static 'jquery-1.12.4.min.js' %&#125;"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="&#123;% static 'bootstrap-3.3.7/js/bootstrap.min.js' %&#125;"&gt;&lt;/script&gt; &#123;% block header_extends %&#125; &lt;!-- 此处可以被其他html文件继承，用于加载css文件 --&gt; &#123;% endblock %&#125;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 栅格系统 类前缀从超小到大分别为.col-xs-、.col-sm-、.col-md-、.col-lg-，也可以使用类如.visible-xs-block、.hidden-xs来显示或者隐藏，使用 例如.col-md-offset- 1类可以将列向右侧偏移。 1234&lt;div class="row"&gt; &lt;div class="col-md-8"&gt;.col-md-8&lt;/div&gt; &lt;div class="col-md-4"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt; 分页基础 shell中可以大致了解分页器的功能： 1234567891011&gt;&gt;&gt;from django.core.paginator import Paginator&gt;&gt;&gt;from blog.models import Blog&gt;&gt;&gt;blogs = Blog.objects.all()&gt;&gt;&gt;paginator = Paginator(blogs, 10)# 常用&gt;&gt;&gt;paginator.count&gt;&gt;&gt;paginator.num_pages&gt;&gt;&gt;paginator.page_range# 赋值&gt;&gt;&gt;page1 = paginator.page(1)&gt;&gt;&gt;page1.object_list 修改views.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def blog_list(request): blogs_all_list = Blog.objects.all() paginator = Paginator(blogs_all_list, settings.EACH_PAGE_BLOGS_NUMBER) # 每10篇为1页 page_num = request.GET.get("page", 1) # 获取url页码参数（GET请求） page_of_blogs = paginator.get_page(page_num) current_page_num = page_of_blogs.number #获取当前页码 # 获取当前页码前后各两位的范围 # page_range = list(range(max(current_page_num - 2, 1), current_page_num)) + \ # list(range(current_page_num, min(current_page_num + 2, paginator.num_pages) + 1)) page_range = [x for x in range(current_page_num-2, current_page_num+3) if (x&gt;0 and x&lt;paginator.num_pages + 1)] # 加上首页和尾页以及省略页码标记 if page_range[0] - 1 &gt;= 2: page_range.insert(0,'...') if paginator.num_pages - page_range[-1] &gt;= 2: page_range.append('...') if page_range[0] != 1: page_range.insert(0,1) if page_range[-1] != paginator.num_pages: page_range.append(paginator.num_pages) context = &#123;&#125; context['blogs'] = page_of_blogs.object_list context['page_of_blogs'] = page_of_blogs context['page_range'] = page_range context['blog_types'] = BlogType.objects.all() # context['blogs_count'] = Blog.objects.all().count() return render_to_response('blog/blog_list.html',context)def blogs_with_type(request, blog_type_pk): blog_type = get_object_or_404(BlogType, pk=blog_type_pk) blogs_all_list = Blog.objects.filter(blog_type=blog_type) paginator = Paginator(blogs_all_list, settings.EACH_PAGE_BLOGS_NUMBER) # 每10篇为1页 page_num = request.GET.get("page", 1) # 获取url页码参数（GET请求） page_of_blogs = paginator.get_page(page_num) current_page_num = page_of_blogs.number #获取当前页码 # 获取当前页码前后各两位的范围 # page_range = list(range(max(current_page_num - 2, 1), current_page_num)) + \ # list(range(current_page_num, min(current_page_num + 2, paginator.num_pages) + 1)) page_range = [x for x in range(current_page_num-2, current_page_num+3) if (x&gt;0 and x&lt;paginator.num_pages + 1)] # 加上首页和尾页以及省略页码标记 if page_range[0] - 1 &gt;= 2: page_range.insert(0,'...') if paginator.num_pages - page_range[-1] &gt;= 2: page_range.append('...') if page_range[0] != 1: page_range.insert(0,1) if page_range[-1] != paginator.num_pages: page_range.append(paginator.num_pages) context = &#123;&#125; context['blog_type'] = blog_type context['blogs'] = page_of_blogs.object_list context['page_of_blogs'] = page_of_blogs context['page_range'] = page_range context['blog_types'] = BlogType.objects.all() # context['blogs_count'] = Blog.objects.all().count() return render_to_response('blog/blogs_with_type.html',context) 修改blog_list.html: 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;div class="paginator"&gt; &lt;ul class="pagination"&gt; &lt;!-- 上一页 --&gt; &lt;li&gt; &#123;% if page_of_blogs.has_previous %&#125; &lt;a href="?page=&#123;&#123; page_of_blogs.previous_page_number &#125;&#125;" aria-label="Previous"&gt; &lt;span aria-hidden="true"&gt;&amp;laquo;&lt;/span&gt; &lt;/a&gt; &#123;% else %&#125; &lt;span aria-hidden="true"&gt;&amp;laquo;&lt;/span&gt; &#123;% endif %&#125; &lt;/li&gt; &lt;!-- 全部页码 --&gt; &#123;% for page_num in page_range %&#125; &#123;% if page_num == page_of_blogs.number %&#125; &lt;li class='active'&gt;&lt;span&gt;&#123;&#123; page_num &#125;&#125;&lt;/span&gt;&lt;/li&gt; &lt;!-- &lt;li class='active'&gt;&lt;a href="?page=&#123;&#123; page_num &#125;&#125;"&gt;&#123;&#123; page_num &#125;&#125;&lt;/a&gt;&lt;/li&gt; --&gt; &#123;% else %&#125; &#123;% if page_num == '...' %&#125; &lt;li&gt;&lt;span&gt;&#123;&#123; page_num &#125;&#125;&lt;/span&gt;&lt;/li&gt; &#123;% else %&#125; &lt;li&gt;&lt;a href="?page=&#123;&#123; page_num &#125;&#125;"&gt;&#123;&#123; page_num &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% endif %&#125; &#123;% endif %&#125; &#123;% endfor %&#125; &lt;!-- 下一页 --&gt; &lt;li&gt; &#123;% if page_of_blogs.has_next %&#125; &lt;a href="?page=&#123;&#123; page_of_blogs.next_page_number &#125;&#125;" aria-label="Next"&gt; &lt;span aria-hidden="true"&gt;&amp;raquo;&lt;/span&gt; &lt;/a&gt; &#123;% else %&#125; &lt;span aria-hidden="true"&gt;&amp;raquo;&lt;/span&gt; &#123;% endif %&#125; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt; 共有&#123;&#123; page_of_blogs.paginator.count&#125;&#125;篇博客， 当前第&#123;&#123; page_of_blogs.number &#125;&#125;页，共&#123;&#123; page_of_blogs.paginator.num_pages &#125;&#125;页 &lt;/p&gt;&lt;/div&gt; 上一页/下一页 编辑views.py，其中__gt表示大于，同理lt为小于： 1234567def blog_detail(request,blog_pk): context = &#123;&#125; blog = get_object_or_404(Blog, pk=blog_pk) context['next_blog'] = Blog.objects.filter(created_time__gt=blog.created_time).last() context['previous_blog'] = Blog.objects.filter(created_time__lt=blog.created_time).first() context['blog'] = blog return render_to_response('blog/blog_detail.html',context)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相似度计算的python实现]]></title>
    <url>%2F2019%2F01%2F03%2F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[在数据挖掘中有很多地方要计算相似度，计算相似度有欧几里德距离、曼哈顿距离、皮尔逊相关度、Jaccard系数和Tanimoto系数等等，这里暂时列出以下几种计算方法。 欧几里得距离 欧几里得距离（Euclidean distance）是一个通常采用的距离定义，指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离），在二维和三维空间中的欧氏距离就是两点之间的实际距离。 在Python中，可以调用scikit-learn中现成的轮子实现： 12from sklearn.metrics.pairwise import euclidean_distancesdistance = euclidean_distances(i1,i2) 因为计算是基于各维度特征的绝对数值，所以欧氏距离需要保证各维度指标在相同的刻度级别，比如对身高（cm）和体重（kg）两个单位不同的指标是不适用于欧氏距离的。 此外，欧几里得距离是数据上的直观体现，在处理一些受主观影响很大的评分数据时，效果则不太明显；比如，person1对item1,item2 分别给出了2分，4分的评价;person2则给出了4分，8分的评分。通过分数可以大概看出，二个人口味相同，但是第二个人评价稍高。在逻辑上，是可以给出两用户兴趣相似度很高的结论。如果此时用欧式距离来处理，则不能得到这样的结论。即评价者的评价相对于平均水平偏离很大的时候欧几里德距离不能很好的揭示出真实的相似度。 曼哈顿距离 曼哈顿距离（Manhattan Distance）又叫出租车距离或者马氏距离，是由十九世纪的赫尔曼·闵可夫斯基所创词汇 ，是种使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和。 对于空间向量(x1,x2,x3,…,xn)和(y1,y2,y3,…,yn)，曼哈顿距离的定义为： 同样可以调用scikit-learn中函数： 12from sklearn.metrics.pairwise import manhattan_distancesdistance = manhattan_distances(i1,i2) 余弦相似度 余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。余弦相似度衡量的是维度间取值方向的一致性，注重维度之间的差异，不注重数值上的差异，而欧氏度量的正是数值上的差异性。 调用scikit-learn中函数: 12from sklearn.metrics.pairwise import cosine_similaritysimilarity = cosine_similarity(i1,i2) 也可以使用cosine_distances函数，他们的关系是：余弦距离= 1 - 余弦相似度。 皮尔逊相关系数 Pearson相关系数是用协方差除以两个变量的标准差得到的，虽然协方差能反映两个随机变量的相关程度（协方差大于0的时候表示两者正相关，小于0的时候表示两者负相关），但其数值上受量纲的影响很大，不能简单地从协方差的数值大小给出变量相关程度的判断。为了消除这种量纲的影响，于是就有了相关系数的概念。 当两个变量的方差都不为零时，相关系数才有意义，相关系数的取值范围为[-1,1]。当相关系数为1时，成为完全正相关；当相关系数为-1时，成为完全负相关；相关系数的绝对值越大，相关性越强；相关系数越接近于0，相关度越弱。 可以使用scipy中现有轮子： 12from scipy.stats import pearsonrcor = pearsonr(i1,i2) 返回为一个元组，第一项为Pearson相关系数，第二项为p-value。 Jaccard系数 Jaccard系数用于比较有限样本集之间的相似性与差异性。当数据集为二元变量时，我们只有两种状态：0或者1，这时候可以使用它来表征相似度。Jaccard系数值越大，样本相似度越高。 在Python中构建函数如下： 1234567891011def getJaccardCoefficient(p1,p2): x1 = p1[p1&gt;0] x2 = p2[p2&gt;0] si = &#123;&#125; for item in list(x1.index): if item in list(x2.index): si[item] = 1 n = len(si) if n == 0: return 0 return n/(len(x1)+len(x2)-n) Tanimoto系数 Tanimoto系数，又称为广义Jaccard系数，Jaccard系数是xy都是二值向量时的特殊情况，在这种情况下Tanimoto系数就等同Jaccard系数。 在Python中构建函数如下： 1234567891011121314def getTanimotoCoefficient(p1,p2): x1 = p1[p1&gt;0] x2 = p2[p2&gt;0] si = &#123;&#125; for item in list(x1.index): if item in list(x2.index): si[item] = 1 n = len(si) if n == 0: return 0 sum1 = sum([pow(it,2) for it in x1]) sum2 = sum([pow(it,2) for it in x2]) sumco = sum([x1[it]*x2[it] for it in si]) return sumco/(sum1+sum2-sumco)]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>Similarity</tag>
        <tag>Euclidean distance</tag>
        <tag>Manhattan distance</tag>
        <tag>Pearson coefficient</tag>
        <tag>Jaccard coefficient</tag>
        <tag>Tanimoto coefficient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些可能用得上的点]]></title>
    <url>%2F2018%2F12%2F26%2F%E4%B8%80%E4%BA%9B%E5%8F%AF%E8%83%BD%E7%94%A8%E5%BE%97%E4%B8%8A%E7%9A%84%E7%82%B9%2F</url>
    <content type="text"><![CDATA[记录一下以后可能用得上的点。 MOE相关Lipinski的五倍律 满足下面标准中的至少两条，则该化合物的被动吸收能力或者膜渗透性将较差，难以通过口服吸收： Weight：相对分子质量大于500Da logP(o/w)：具有高亲脂性即clgP&gt;5 lip_don：氢键供体数大于5（—OH和—NH之和） lip_acc：氢键受体数大于10（O原子和N原子之和） b_rotN：含有多于5个可旋转键，以限制其构象柔性 此外，moe还有一项lip_violation，以提示Lipinski五倍律违反数。 以多mpu运行 CMD下运行，%MOE%为MOE根目录： 12%MOE%\bin\moe -mpu 2// 数字为调用mpu数目 计算cats描述符 SVL输入： 12db_cats [,3]// 后面为切片，以1为起始 RDKit相关计算Morgan指纹 当radius=2时，即为ECFP4 ，nBits为长度。 123from rdkit import Chemm1 = Chem.MolFromSmiles('Cc1ccccc1')fp1 = Chem.AllChem.GetMorganFingerprintAsBitVect(m1,2,nBits=1024)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Cheminformatics</tag>
        <tag>MOE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2018%2F12%2F25%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最近打算系统的回顾一下机器学习算法，所以以《机器学习实战》为依据，这次就由k-近邻算法开始回顾。 k-近邻算法概述 k-近邻算法（K-Nearest Neighbor ）的工作原理是：存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的新数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 以使用k-近邻算法分类爱情片和动作片，有人统计过很多电影的打斗镜头和接吻镜头如下表，假如有一部未看过的电影，以？来表示，如何确定它是爱情片和动作片呢？ 电影名称 打斗镜头 接吻镜头 电影类型 Califoria Man 3 104 爱情片 He’s Not Really into Dudes 2 100 爱情片 Beautigul Woman 1 81 爱情片 Kevin Longblade 101 10 动作片 Robo Slayer 3000 99 5 动作片 Amped II 98 2 动作片 ? 18 90 未知 在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离： 我们用欧氏距离来得到如下距离值： 电影名称 与未知电影的距离 Califoria Man 20.5 He’s Not Really into Dudes 18.7 Beautigul Woman 19.2 Kevin Longblade 115.3 Robo Slayer 3000 117.4 Amped II 118.9 按照距离递增排序，可以找到k个距离最近的电影。假设k=3，则三个最靠近的电影依次是He’s Not Really into Dudes、Beautigul Woman和Califoria Man。k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。 准备：使用Python导入数据 首先，创建名为KNN.py的python模块： 1234567import numpy as npimport operatordef createDataSet(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group, labels 这里我们准备了4组数据，每组数据有两个我们已知的属性或者特征值。接下来我们将完成分类任务。 实施KNN分类算法 定义函数classify0()如下： 1234567891011121314151617def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] #用于计算欧氏距离 #将intX在纵向重复dataSetSize次 diffMat = np.tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 #返回从小到大的索引值 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 #按第二个元素的次序，对元组进行逆序排序，即从大到小排序，返回发生频率最高的元素标签 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] 至此，我们构建了第一个分类器。 示例：使用k-近邻算法改进约会网站的配对效果 海伦一直使用在线约会网站寻找自己的约会对象，她把自己交往过的对象分为以下三类： 不喜欢的人 魅力一般的人 极具魅力的人 因此，她希望更好地将她匹配的对象分类。 准备数据：从文本文件中解析数据 她把数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，主要包含以下3种特征： 每年获得的飞行常客里程数 玩视频游戏所耗时间百分比 每周消费的冰淇淋公升数 创建名为file2matrix的函数，把数据转变为分类器可以接受的格式。 1234567891011121314151617def file2matrix(filename): fr = open(filename) arrayOlines = fr.readlines() # 得到文件行数 numberOfLines = len(arrayOlines) # 创建返回的NumPy矩阵 returnMat = np.zeros((numberOfLines,3)) classLabelVector = [] index = 0 # 解析文件数据到列表 for line in arrayOlines: line = line.strip() listFromLine = line.split('\t') returnMat[index,:] = listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat,classLabelVector 接下来图形化展示数据内容。 分析数据：使用Matplotlib创建散点图 定义一个scatterPlot函数进行可视化。 12345678import matplotlibimport matplotlib.pyplot as pltdef scatterPlot(): fig = plt.figure() ax = fig.add_subplot(111) p = ax.scatter(datingDateMat[:,1], datingDateMat[:,2], 15.0*np.array(datingLabels), 15.0*np.array(datingLabels)) plt.show() 得到了玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数的关系散点图： 同理也能得到每年获得的飞行常客里程数和玩视频游戏所耗时间百分比的关系散点图，可以得到更好的展示效果： 可以知道具有不同的爱好的人其类别区域也不同。 准备数据：归一化数值 玩视频游戏所耗时间百分比 每年获得的飞行常客里程数 每周消费的冰淇淋公升数 样本分类 1 0.8 400 0.5 1 2 12 134000 0.9 3 3 0 20000 1.1 2 4 67 32000 0.1 2 上表给出提取的四组数据，要计算样本3和4的距离，使用以下方法： 容易发现，上面方程中数值差值最大的属性对计算结果的影响最大，也就是说，每年获取的飞行常客里程数对于计算结果的影响将远远大于该表其他两个特征的影响。而三种特征是同等重要的，因此需要把数值归一化。 增加函数autoNorm用于归一化： 123456789def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDateSet = np.zeros(np.shape(dataSet)) m = dataSet.shape[0] normDateSet = dataSet - np.tile(minVals, (m,1)) normDateSet = normDateSet/np.tile(ranges, (m,1)) return normDateSet, ranges, minVals 在机器学习框架scikit-learn中，有封装好的标准化函数： 1234567891011# 归一化from sklearn.preprocessing import MinMaxScalerss = MinMaxScaler()X = ss.fit_transform(X)# 标准化from sklearn.preprocessing import StandardScalerss = StandardScaler()X = ss.fit_transform(X)#归一化其实就是标准化的一种方式，只不过归一化是将数据映射到了[0,1]这个区间中。#标准化则是将数据按照比例缩放，使之放到一个特定区间中。标准化后的数据的均值＝0，标准差＝1，因而标准化的数据可正可负。 测试算法：作为完整程序验证分类器 创建函数datingClassTest以测试分类器效果： 123456789101112def datingClassTest(): hoRatio = 0.10 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') normMat, ranges, minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, datingLabels[i])) if (classifierResult != datingLabels[i]): errorCount += 1.0 print("the total error rate is: %f" % (errorCount/float(numTestVecs))) 通过分类器可得错误率为5%，结果还不错，也可以改变hoRatio和k来得到不同结果。 这个例子表明我们可以得到较好的分类结果，海伦可以输入未知对象的属性，由分类软件来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。 示例：手写识别系统 上面的例子使用的数据比较容易理解，如何在人不太容易看懂的数据上使用分类器呢？我们将使用k-近邻分类器用于手写数字识别。 准备数据：将图像转换为测试向量 首先把图像格式化处理为一个向量。即将32×32的二进制文本。如果要使用我们的分类器，就将其变成1×1024的向量。 编写函数实现： 12345678def img2vector(filename): returnVect = np.zeros((1,1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect 测试算法：使用k-近邻算法识别手写数字 使用以下代码测试分类器： 1234567891011121314151617181920212223hwLabels = []trainingFileList = os.listdir('trainingDigits') # 读取训练集m = len(trainingFileList)trainingMat = np.zeros((m,1024))for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] # 从文件名解析分类数字 classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr)testFileList = os.listdir('testDigits') # 载入测试集errorCount = 0.0mTest = len(testFileList)for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)) if (classifierResult != classNumStr): errorCount += 1.0print("\nthe total number of errors is: %d" % errorCount)print("\nthe total error rate is: %f" % (errorCount/float(mTest))) 本函数是将trainingDigits目录中的文件内容存储在列表中，然后得到目录中有多少文件，并将其存储到变量m中。接着，创建m行1024列的训练矩阵，该矩阵的每行数据存储一个图像。我们可以从文件名中解析出分类数字。该目录下的文件按照规则命名，如文件9_45.txt的分类是9，它是数字9的第45个实例。然后我们可以将类代码存储到hwLabels向量中，使用前面讨论的img2vector函数载入图像。在下一步中，我们对testDigits目录中的文件执行相似的操作，不同之处是我们并不将这个目录下的文件载入矩阵中，而是使用classify0()函数测试该目录下的每个文件。由于文件中的值已经在0和1之间，并不需要进行标准化。 通过测试本分类器，发现有相对理想的结果。 实际使用本算法时，算法的执行效率并不高。因为算法需要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行9000次，此外还得为测试向量准备2MB的存储空间。是否存在一种算法减少存储空间和计算时间的开销呢？k决策树就是k-近邻算法的优化版，可以节省大量计算开销。 本章小结 k-近邻算法是分类数据最简单最有效的算法，本章通过两个例子蒋旭了如何使用k-近邻算法构造分类器。k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。 k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。可以通过概率测量方法处理分类问题以解决此问题。 参考资料：《机器学习实战》]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NS开箱]]></title>
    <url>%2F2018%2F07%2F24%2FNS%E5%BC%80%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[想买NS很久了，之前一直听说NS品控不好，还存在变弯风险。所以想等第二代，不过最近价格回落，正好把steam套了点现，就剁手了NS。感觉近来一直在放飞自我，没好好学习，十分惭愧，希望从今天开始好好复习…… 开箱 大概是周六晚上下的单，等了一天多点到货，盒子很朴素： 拆开就能看到主机和2个Joy-Con： 全家福，还有电源、2个腕带、握把、HDMI连接线和底座： 游戏买了个《异度神剑2》的卡带，以及一个《一起剪吧！剪纸狙击手！》，买完才意识到并没有小伙伴一块玩，肥宅留下了悲伤的眼泪。 上个卡带图： 听说卡带是苦的，并不敢去试…… 留个资料页，欢迎有缘人来加好友： 总结 相比于我的3DS，手感要强不少，但是总感觉按键很松。 充电口在下面，有点反人类。另外贴膜没贴好，果然不应该自己贴膜的……老任的做工确实很烂，希望我的NS不会弯吧~]]></content>
      <categories>
        <category>生活记录</category>
      </categories>
      <tags>
        <tag>Unboxing</tag>
        <tag>Nintendo</tag>
        <tag>Switch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AirPods开箱]]></title>
    <url>%2F2018%2F05%2F29%2FAirPods%E5%BC%80%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[懒癌发作，好久没更博客了，发一篇开箱（伪）凑数吧…… 一直挺想用蓝牙耳机的，之前在索尼的降噪豆和AirPods之间纠结，考虑到对于降噪需求并不是很大，所以最终还是选了AirPods，就是喜欢白色的，颜值即正义。 首先是盒子： 打开后来个全家福，正好之前的原装数据线坏了，可以做个备用。必须吐槽苹果的垃圾原装线，用久了要么就是一面不能充电，要么就是开胶，质量还不如几十块的。 盒子内： 拿出来： 说下使用体会吧，感觉上还是挺好用的，戴着很舒服，不容易掉。和Iphone完美契合，配对很方便，使用中拿下一边耳机自动暂停播放，放回去自动开始。虽然只能设置简单的控制，对我来说基本够用。连接电脑声音有点小，得调很大才行，这点不太好，勉强可以接受吧。至于音质，也就听个响的样子，不能指望有多强了~ 希望不会丢……]]></content>
      <categories>
        <category>生活记录</category>
      </categories>
      <tags>
        <tag>Unboxing</tag>
        <tag>Earphone</tag>
        <tag>Apple</tag>
        <tag>AirPods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类似“超强智商”之类的答题游戏作弊思路]]></title>
    <url>%2F2018%2F05%2F08%2F%E7%B1%BB%E4%BC%BC%E2%80%9C%E8%B6%85%E5%BC%BA%E6%99%BA%E5%95%86%E2%80%9D%E4%B9%8B%E7%B1%BB%E7%9A%84%E7%AD%94%E9%A2%98%E6%B8%B8%E6%88%8F%E4%BD%9C%E5%BC%8A%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[今年年初时答题游戏比较火，对于一些比较热门的游戏，已经有不少成熟的辅助程序了。然而这个实验室同学转给我的这个小程序，似乎热度还达不到做辅助的标准，而题目又比较恶心，自己答起来比较费劲，所以在这里探讨一下作弊的思路。 利用图像识别的思路 很容易想到的一个思路就是把手机屏幕投射到屏幕，这里可以用模拟器/ADB/TeamViewer等方案，然后截图，利用tesseract-ocr将图片识别成文字，最后调用百度搜题。 安装必要库就不赘叙了，有需要日后再补。我是采用了模拟器的方案，简单写出代码如下： 12345678910111213141516171819202122import win32gui, win32ui, win32con, win32apifrom PIL import Imageimport pytesseractimport webbrowserdef window_capture(filename): hwnd = 0 hwndDC = win32gui.GetWindowDC(hwnd) mfcDC = win32ui.CreateDCFromHandle(hwndDC) saveDC = mfcDC.CreateCompatibleDC() saveBitMap = win32ui.CreateBitmap() w = 480 h = 120 saveBitMap.CreateCompatibleBitmap(mfcDC,w,h) saveDC.SelectObject(saveBitMap) saveDC.BitBlt((0,0),(w,h),mfcDC,(50,320),win32con.SRCCOPY) saveBitMap.SaveBitmapFile(saveDC,filename)window_capture('cqzs.jpg')text=pytesseract.image_to_string(Image.open('cqzs.jpg'),lang='chi_sim')new_text =''.join(text.split())url = 'http://www.baidu.com/s?wd=%s' % new_textwebbrowser.open(url) 其中w，h为截图框大小参数，还需要根据实际情况调整定位，以便完整截图。 实际使用情况图片识别大致能让人满意，然而效率上有点低，整个过程大概要3-5秒，只剩一半左右的时间答题，所以希望有更好的方案。 利用抓包的思路 通过Fiddler抓包发现，每次12道题以json的形式一次性直接传递过来，因此可以在答题倒计时时就直接把12道题一块搜，这样每道题基本上有10秒时间作答，比上面的方案充裕了不少。 安装的过程略，需要修改FiddlerScript使其输出题目。代码如下： 123456789101112131415161718import jsonimport webbrowserstat = Truewhile stat == True: try: file = open("d:\\python\\temp\\cqzs.txt","r",encoding="utf-8") j = json.loads(file.read()) if 'game' in j['data']: question_list = j['data']['game']['question_list'] for q in question_list: title = q['title'] url = 'http://www.baidu.com/s?wd=%s' % title webbrowser.open(url) stat = False file.close() except: pass 进一步改进方案 这样只是把每道题用百度搜了一下，还需要人工判断，可以考虑用通过词频比对选择答案，或者多平台搜索集合之类的方案。以后有机会再补。 结局 奖品虽然最低只需要通关5次，但是不包邮，强烈怀疑奖品价值够不上邮费。想免邮费最低需要通关25次，考虑到麻烦程度，溜了溜了~]]></content>
      <categories>
        <category>不务正业</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pytesseract</tag>
        <tag>Fiddler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将SMILES转成InChiKey]]></title>
    <url>%2F2018%2F04%2F21%2F%E5%B0%86SMILES%E8%BD%AC%E6%88%90InChiKey%2F</url>
    <content type="text"><![CDATA[最近在项目中需要将SMILES转成InChiKey，通常可以在OpenBabelGUI中操作完成，但是对于大量数据在OpenBabelGUI中操作就有些不方便且容易失误。这里记录一下我解决这个问题的过程。 安装相关包 先建立一个虚拟环境，安装RDKit： 1conda create -c rdkit -n my-rdkit-env rdkit 然后在虚拟环境下安装OpenBabel： 1conda install -n my-rdkit-env -c openbabel openbabel 出现的问题 最初想用Pybel实现，然而发现Pybel并不支持输出格式为InChiKey，既然这条路走不通，就换个方案吧。于是想查看OpenBabel的文档看看能不能实现，然而发现这玩意的python接口只是寥寥几言，也没有具体的文档。不过Google到了一个轮子写了如何将SMILES转成InChiKey，代码如下： 1234567import openbabel as obconv = ob.OBConversion()conv.SetInAndOutFormats("smi", "inchi")conv.SetOptions("K", conv.OUTOPTIONS)mol = ob.OBMol()conv.ReadString(mol, "CC(=O)Cl")inchikey = conv.WriteString(mol): 本想着问题解决了，不过试了一下发现并不能用。而且可耻的找不到原因…… 不过也问题不大，RDKit可以把SMILES转成InChi，然后再把InChi变成InChiKey就行了，于是写了个函数实现: 1234567from rdkit import Chemfrom rdkit.Chem.inchi import rdinchidef smiletoinchikey(smile): mol = Chem.MolFromSmiles(smile) inchi = rdinchi.MolToInchi(mol) inchikey = rdinchi.InchiToInchiKey(inchi[0]) return inchikey 然而在处理项目文件时发现，有些SMILES式RDKit不能识别，导致程序出错。如Clc1c([C@@H]2[C@@H]([N+H3])CC(CN3CCC(C(=O)O)CC3)=CC2)ccc(Cl)c1，于是写了个函数利用Pybel将其转成Canonical SMILES。最后还是有一些剩余问题，比如Oc1ccc([C+2]2345[B-2]678[B-3]9%10%11([C-3])[C+]%12%13%14[B-2]%15%169[B-2]26%10[B-2]23%15[B-2]364[B-2]457[B-2]8%11%12[B-2]%1334[B-2]%14%1626)cc1转成Canonical SMILES依然无法读取，又或者O=BOB(OB(OB=O)[O-])[O-]没转换前能读取，转换后不能读取，只能按例外处理。 最后代码如下： 123456789101112131415161718192021222324#!/usr/bin/env python3# -*- coding: utf-8 -*-# @Time : Thu Apr 20 16:12:41 2018# @Author : Catkin# @Website : blog.catkin.moeimport pybelfrom rdkit import Chemfrom rdkit.Chem.inchi import rdinchidef smitosmile(smi): mol = pybel.readstring("smi", smi) smile = mol.write('can') smile = smile.replace('\t\n', '') return smile def smiletoinchikey(smile): mol = Chem.MolFromSmiles(smile) if mol is None: smi = smitosmile(smile) mol = Chem.MolFromSmiles(smi) inchi = rdinchi.MolToInchi(mol) inchikey = rdinchi.InchiToInchiKey(inchi[0]) return inchikey 用OpenBabel同样也可以把SMILES转成Canonical SMILES： 12345678910import openbabel as obdef obsmitosmile(smi): conv = ob.OBConversion() conv.SetInAndOutFormats("smi", "can") conv.SetOptions("K", conv.OUTOPTIONS) mol = ob.OBMol() conv.ReadString(mol, smi) smile = conv.WriteString(mol) smile = smile.replace('\t\n', '') return smile]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OpenBabel</tag>
        <tag>Pybel</tag>
        <tag>RDKit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Aero15x开箱]]></title>
    <url>%2F2018%2F04%2F17%2FAero15x%E5%BC%80%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[之前一直在gs65和aero15x中摇摆不定，在已经付了gs65的定金后还是选择了aero15x。主要原因就是aero15x比较好拆，实在不想装个内存、固态或者清灰都跑一趟售后，于是就损失了100定金~ \ (╯-╰) / 16号凌晨下单，等了一天半到手。先来个箱子图： 全部部件，这个适配器真是大，跟块板砖差不多，另外这个光盘是闹哪样，这个本子有光驱吗？自家的AORUS都用U盘了，赢刃还没跟上，真是无力吐槽。 外观，整机是全金属的，感觉做工还算可以，也能单手开合，就是有点指纹收集器的感觉。个人不喜欢太浮夸的外观，这样感觉刚刚好。 下了个娱乐大师，看一眼配置： 之前看别人的拆机内存是芝奇，显示器是LG的，到我变成金士顿和友达，总有点被缩水的感觉…… 跑个分： 由于有后台，跑分比实际水平大概低一点。 来张正面图。 接下来是使用体验：确实和网上说的，有一些小毛病，比如右shift不是很亮，USB接口太紧，然后有时候键盘会双击，强迫症应该会很难受。关键是这些问题去年就有了，到现在也没能解决……至于散热，全靠暴力扇，跑个程序风扇起飞，噪音还是很大的。不过轻薄游戏本要什么自行车，对我来说还是可以接受了。也就不当摸摸党了~]]></content>
      <categories>
        <category>生活记录</category>
      </categories>
      <tags>
        <tag>Unboxing</tag>
        <tag>Gigabyte</tag>
        <tag>Laptop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FP-growth算法]]></title>
    <url>%2F2018%2F02%2F06%2FFP-growth%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[你用过搜索引擎吗？输入一个单词或者一个单词的一部分，搜索引擎就会自动补全查询词项。用户甚至事先都不知道搜索引擎推荐的东西是否存在，反而会去查找推荐词项。那么这些推荐词项是如何被搜索引擎找到的？那是因为研究人员使用了FP-growth算法，它是基于Apriori构建的，但完成相同任务时将数据集存储在一个特定的称作FP树的结构之后发现频繁项集或频繁项对，即常在一起出现的元素项的集合FP树。这是一种比Apriori执行速度更快的算法，能高效地发现频繁项集，但不能用于发现关联规则。 FP-growth算法只需要对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁，因此FP-growth算法的速度比Apriori算法快。在小规模数据上，这不是什么问题，但当处理更大数据集时，就会产生较大问题。FP-growth算法只会扫描数据集两次，它不会生成候选项集，其发现频繁项集的基本过程如下： 构建FP树 从FP树中挖掘频繁项集 FP-growth原理 FP-growth算法将数据存储在一种成为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一颗FP树与计算机科学中其他树结构类似，但它通过链接（link）来连接相似元素，被连起来的元素可以看成一个链表。 下面用一个例子来说明，给出一组数据如下： 事物ID 事物中的元素项 001 r,z,h,j,p 002 z,y,x,w,v,u,t,s 003 z 004 r,x,n,o,s 005 y,r,x,z,q,t,p 006 y,z,x,e,q,s,t,m 由此可以生成一棵FP树： 仔细来看这棵FP树，同搜索树不同，一个元素项可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中单个元素及其在序列中的出现次数，路径会给出该序列的出现次数。相似项之间的链接，即节点链接（node link），用于快速发现相似项的位置。 上图中，元素项z出现了5次，集合{r, z}出现了1次。于是可以知道：z一定是自己本身或者和其他符号一起出现了4次。再看看其他可能，集合{t, s, y, x, z}出现了2次，集合{t, r, y, x, z}出现了1次。元素项z的右边是5，表示z出现了5次，其中刚才已经给出了4次出现，所以它一定单独出现过1次。005号记录是{y, r, x, z, q, t, p}，那么q、p去哪儿了呢？ 这里使用第11章给出的支持度定义，该指标对应一个最小阈值，低于最小阈值的元素项被认为是不频繁的。若将最小支持度设为3，然后应用频繁项分析算法，就会获得出现3次或3次以上的项集。图中FP树的最小支持度是3，因此p、q并没有出现在树中。 FP-growth算法的工作流程：首先构建FP树，然后利用它来挖掘频繁项集。为了构建FP树，需要对原始数据集扫描两遍。第一遍对所有元素项的出现次数进行计数。记住Apriori原理：如果某元素是不频繁的，那么包含该元素的超集也是不频繁的，所以就不需要考虑超集。数据库的第一遍扫描用来统计出现的频率，第二遍扫描中只考虑那些频繁元素。 构建FP树 首先先用一个容器来保存FP树。 创建FP树的数据结构 创造一个类保存树的每个节点，创建文件fpGrowth.py并加入以下代码： 123456789101112131415161718192021class treeNode : def __init__(self, nameValue, numOccur, parentNode) : # 节点名称 self.name = nameValue self.count = numOccur # 用于链接相似的元素项 self.nodeLink = None # 当前节点的父节点 self.parent = parentNode # 用于存放节点的子节点 self.children = &#123;&#125; # 对count变量增加给定值 def inc(self, numOccur) : self.count += numOccur # 将树以文本的形式显示 def disp(self, ind=1) : print(' '*ind, self.name, ' ', self.count) for child in self.children.values() : child.disp(ind+1) 上面的程序给出了FP树中节点的类定义。类中包含用于存放节点名字的变量和1个计数值，nodeLink变量用于链接相似的元素项（参考上图中的虚线）。类中还使用了父变量parent来指向当前节点的父节点。通常情况并不需要这个变量，因为通常是从上往下迭代访问节点的。如果需要根据给定叶子节点上溯整棵树，这时候就需要指向父节点的指针。最后，类中还包含一个空字典变量，用于存放节点的子节点。 运行一下如下代码： 1234567891011121314&gt;&gt;&gt; import fpGrowth&gt;&gt;&gt; rootNode = fpGrowth.treeNode('pyramid',9,None)# 已创建一个单节点，接下来增加一个子节点&gt;&gt;&gt; rootNode.children['eye'] = fpGrowth.treeNode('eye',13,None)# 显示子节点&gt;&gt;&gt; rootNode.disp() pyramid 9 eye 13# 再添加一个节点&gt;&gt;&gt; rootNode.children['phoenix']=fpGrowth.treeNode('phoenix',3,None)&gt;&gt;&gt; rootNode.disp() pyramid 9 eye 13 phoenix 3 构建FP树 除上图给出的FP树之外，还需要一个头指针表来指向给定类型的第一个实例。利用头指针表，可快速访问FP树中一个给定类型的所有元素。下图给出了一个头指针表的示意图。 这里使用一个字典作为数据结构来保存头指针表。除了存放指针外，头指针表还可以用来保存FP树中每类元素的总数。 第一次遍历数据集会获得每个元素项的出现频率。接着去掉不满足最小支持度的元素项，再来构建FP树。在构建时，读入每个项集并将其添加到一条已经存在的路径中。如果该路径不存在，则创建一条新路径。每个事务就是一个无序集合。假设有集合{z, x, y}和{y, z, r}，那么在FP树中，相同项会只表示一次。为了解决此问题，在将集合添加到树之前，需要对每个集合进行排序。排序基于元素项的绝对出现频率来进行。使用上图中头指针节点值，对前表中数据进行过滤、重排序后的数据显示如下。 事务ID 事务中的元素项 过滤及重排序后的事务 001 r, z, h, j, p z, r 002 z, y, x, w, v, u, t, s z, x, y, s, t 003 z z 004 r, x, n, o, s x, s, r 005 y, r, x, z, q, t, p z, x, y, r, t 006 y, z, x, e, q, s, t, m z, x, y, s, t 在对事务记录过滤和排序之后，就可以构建FP树了。从空集（符号为∅）开始，向其中不断添加频繁项集。过滤、排序后的事务依次添加到树中，如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝。对上表前两条事务进行添加的过程显示如下。 接下来通过代码实现上述过程。在fpGrowth.py加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# FP树构建函数# 使用数据集以及最小支持度作为参数来构建FP树，树构建过程会遍历数据集两次def createTree(dataSet, minSup=1) : headerTable = &#123;&#125; # 第一次遍历扫描数据集并统计每个元素项出现的频度，这些信息被保存在头指针中 for trans in dataSet : for item in trans : headerTable[item] = headerTable.get(item, 0) + dataSet[trans] # 接着扫描头指针表删除那些出现次数小于minSup的项，由于字典不能在遍历中修改，转成集合 for k in list(headerTable.keys()): if headerTable[k] &lt; minSup : del(headerTable[k]) freqItemSet = set(headerTable.keys()) # 如果所有项都不频繁，无需下一步处理 if len(freqItemSet) == 0 : return None, None # 对头指针表稍加扩展以便可以保存计数值及指向每种类型第一个元素项的指针 for k in headerTable : headerTable[k] = [headerTable[k], None] # 创建只包含空集合的根节点 retTree = treeNode('Null Set', 1, None) for tranSet, count in dataSet.items() : localD = &#123;&#125; # 根据全局频率对每个事务中的元素进行排序 for item in tranSet : if item in freqItemSet : localD[item] = headerTable[item][0] if len(localD) &gt; 0 : orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p : p[1], reverse=True)] # 排序后，调用updateTree()方法 updateTree(orderedItems, retTree, headerTable, count) return retTree, headerTable# 为了让FP树生长，需调用updateTree函数。def updateTree(items, inTree, headerTable, count) : # 该函数首先测试事务中的第一个元素项是否作为子节点存在。 if items[0] in inTree.children : # 如果存在，则更新该元素项的计数 inTree.children[items[0]].inc(count) else : # 如果不存在，则创建一个新的treeNode并将其作为一个子节点添加到树中，这时，头指针表也要更新以指向新的节点。 inTree.children[items[0]] = treeNode(items[0], count, inTree) if headerTable[items[0]][1] == None : headerTable[items[0]][1] = inTree.children[items[0]] else : # 更新头指针表需要调用函数updateHeader updateHeader(headerTable[items[0]][1], inTree.children[items[0]]) # updateTree()完成的最后一件事是不断迭代调用自身，每次调用时会去掉列表中的第一个元素 if len(items) &gt; 1 : updateTree(items[1::], inTree.children[items[0]], headerTable, count)# 确保节点链接指向树中该元素项的每一个实例，从头指针的nodeLink开始，一直沿着nodeLink直到到达链表末尾。# 当处理树的时候，一种自然的反应就是迭代完整每一件事。当以相同方式处理链表时可能会遇到一些问题，# 原因是如果链表很长可能会遇到迭代调用的次数限制def updateHeader(nodeToTest, targetNode) : while (nodeToTest.nodeLink != None) : nodeToTest = nodeToTest.nodeLink nodeToTest.nodeLink = targetNode# 载入数据集def loadSimpDat() : simpDat = [ ['r', 'z', 'h', 'j', 'p' ], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's' ], ['z' ], ['r', 'x', 'n', 'o', 's' ], ['y', 'r', 'x', 'z', 'q', 't', 'p' ], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm' ] ] return simpDat# 从列表向字典的类型转换def createInitSet(dataSet) : retDict = &#123;&#125; for trans in dataSet : retDict[frozenset(trans)] = 1 return retDict 运行结果： 123456789101112131415161718192021222324&gt;&gt;&gt; import fpGrowth&gt;&gt;&gt; simpDat = fpGrowth.loadSimpDat()&gt;&gt;&gt; simpDat[['r', 'z', 'h', 'j', 'p'], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'], ['z'], ['r', 'x', 'n', 'o', 's'], ['y', 'r', 'x', 'z', 'q', 't', 'p'], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]&gt;&gt;&gt; initSet = fpGrowth.createInitSet(simpDat)&gt;&gt;&gt; initSet&#123;frozenset(&#123;'j', 'z', 'h', 'p', 'r'&#125;): 1, frozenset(&#123;'u', 's', 'x', 'w', 'y', 'v', 'z', 't'&#125;): 1, frozenset(&#123;'z'&#125;): 1, frozenset(&#123;'s', 'x', 'o', 'n', 'r'&#125;): 1, frozenset(&#123;'q', 'x', 'y', 'z', 't', 'p', 'r'&#125;): 1, frozenset(&#123;'s', 'e', 'q', 'x', 'y', 'z', 'm', 't'&#125;): 1&#125;&gt;&gt;&gt; myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 3)&gt;&gt;&gt; myFPtree.disp() Null Set 1 z 5 r 1 x 3 s 2 y 2 t 2 y 1 t 1 r 1 x 1 s 1 r 1 上面给出的是元素项及其对应的频率计数值，其中每个缩进表示所处的树的深度。 从一棵FP树中挖掘频繁项集 有了FP树之后，就可以抽取频繁项集了。这里的思路与Apriori算法大致类似，首先从单元素项集合开始，然后在此基础上逐步构建更大的集合。当然这里将利用FP树来做实现上述过程，不再需要原始数据集了。从FP树中抽取频繁项集的三个基本步骤如下： 从FP树中获得条件模式基； 利用条件模式基，构建一个条件FP树； 迭代重复步骤1、2，直到树包含一个元素项为止。 重点关注第1步，即寻找条件模式基的过程。之后，为每一条件模式基创建对应的条件FP树。最后需构造少许代码来封装上述两个函数，并从FP树中获得频繁项集。 抽取条件模式基 首先从之前保存在头指针中的单个频繁元素项开始。对于每一个元素项，获得其对应的条件模式(conditional pattern base)。条件模式基是以所查找元素项为结尾的路径集合。每一条路径其实都是一条前缀路径(prefix path)。简而言之，一条前缀路径是介于所查找元素项与树根节点之间的所有内容。 回到图2中，符号r的前缀路径是{x, s}、{z, x, y}和{z}。每条前缀路径都与一个计数值关联。该计数值等于起始元素项的计数值，该计数值给了每条路径上r的数目。下表列出了上例当中每一个频繁项的所有前缀路径： 频繁项 前缀路径 z {}5 r {x, s}1, {z, x, y}1, {z}1 x {z}3, {}1 y {z, x}3 s {z, x, y}2, {x}1 t {z, x, y, s}2, {z, x, y, r}1 前缀路径被用于构建条件FP树，但是暂时先不需要考虑这件事。为了获得这些前缀路径，可以对树进行穷举式搜索，直到获得想要的频繁项为止，或使用一个更有效的方法来加速搜索过程。可以利用先前创建的头指针来得到一种更有效的方法。头指针表包含相同类型元素链表的起始指针。一旦到达了每一个元素项，就可以上溯这棵树直到根节点为止。下面代码给出了如何发现前缀路径，将其添加到文件fpGrowth.py中。 1234567891011121314151617def ascendTree(leafNode, prefixPath) : # 迭代上溯整棵树 if leafNode.parent != None : prefixPath.append(leafNode.name) ascendTree(leafNode.parent, prefixPath)# 遍历链表直到到达结尾。每遇到一个元素项都会调用ascendTree()来上溯FP树，并收集所有遇到的元素项的名称。# 该列表返回之后添加到条件模式基字典condPats中def findPrefixPath(basePat, treeNode) : condPats = &#123;&#125; while treeNode != None : prefixPath = [] ascendTree(treeNode, prefixPath) if len(prefixPath) &gt; 1 : condPats[frozenset(prefixPath[1:])] = treeNode.count treeNode = treeNode.nodeLink return condPats 运行结果 123456&gt;&gt;&gt; fpGrowth.findPrefixPath('x', myHeaderTab['x'][1])&#123;frozenset(&#123;'z'&#125;): 3&#125;&gt;&gt;&gt; fpGrowth.findPrefixPath('z', myHeaderTab['z'][1])&#123;&#125;&gt;&gt;&gt; fpGrowth.findPrefixPath('r', myHeaderTab['r'][1])&#123;frozenset(&#123;'z'&#125;): 1, frozenset(&#123;'s', 'x'&#125;): 1, frozenset(&#123;'z', 't', 'y', 'x'&#125;): 1&#125; 创建条件FP树 对于每个频繁项，都要创建一棵条件FP树。我们会为z、x以及其他频繁项构建条件树。可以使用刚才发现的条件模式基作为输入数据，并通过相同的建树代码来构建这些树。然后，我们会递归地发现频繁项、发现条件模式基，以及发现另外的条件树。举个例子，假定为频繁项 t 创建一个条件FP树，然后对{t, y}、{t, x}、……重复该过程。元素项t的条件FP树的构建过程如下所示。 上图中最初树以空集作为根节点，接着，原始的集合{y, x, s, z}中的集合{y, x, z}被添加进来。因为不满足最小支持度要求，字符s并没有加入进来。类似地，{y, x, z}也从原始集合{y, x, r, z}中添加进来。 元素项s、r是条件模式基的一部分，但它们并不属于条件FP树。单独来看它们都是频繁项，但是在t的条件树中，它们却不是频繁的，也就是说{t, r}、{t, s}是不频繁的。 接下来，对集合{t, z}、{t, x}、{t, y}来挖掘对应的条件树。这会产生更复杂的频繁项集。该过程重复进行，直到条件树中没有元素为止，然后就可以停止了。实现代码很直观，使用一些递归加上之前写的代码即可。具体如下： 12345678910111213141516171819def mineTree(inTree, headerTable, minSup, preFix, freqItemList) : # 对头指针表中元素项按照其出现频率进行排序，默认是从小到大 bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p:p[1][0])] # 默认是从小到大，下面过程是从头指针的底端开始 for basePat in bigL : newFreqSet = preFix.copy() newFreqSet.add(basePat) # 将每个频繁项添加到频繁项集列表freqItemList中 freqItemList.append(newFreqSet) # 使用findPrefixPath()创建条件基 condPattBases = findPrefixPath(basePat, headerTable[basePat][1]) # 将条件基condPattBases作为新数据集传递给createTree()函数 # 这里为函数createTree()添加足够的灵活性，确保它可以被重用于构建条件树 myCondTree, myHead = createTree(condPattBases, minSup) # 如果树中有元素项的话，递归调用mineTree()函数 if myHead != None : print('conditional tree for: ', newFreqSet) myCondTree.disp() mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList) 效果如下： 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; freqItems = []&gt;&gt;&gt; fpGrowth.mineTree(myFPtree, myHeaderTab, 3, set([]), freqItems)conditional tree for: &#123;'s'&#125; Null Set 1 x 3conditional tree for: &#123;'y'&#125; Null Set 1 z 3 x 3conditional tree for: &#123;'y', 'x'&#125; Null Set 1 z 3conditional tree for: &#123;'t'&#125; Null Set 1 z 3 y 3 x 3conditional tree for: &#123;'t', 'y'&#125; Null Set 1 z 3conditional tree for: &#123;'t', 'x'&#125; Null Set 1 y 3 z 3conditional tree for: &#123;'z', 't', 'x'&#125; Null Set 1 y 3conditional tree for: &#123;'x'&#125; Null Set 1 z 3# 检查返回的项集是否与条件树匹配&gt;&gt;&gt; freqItems[&#123;'r'&#125;, &#123;'s'&#125;, &#123;'s', 'x'&#125;, &#123;'y'&#125;, &#123;'y', 'z'&#125;, &#123;'y', 'x'&#125;, &#123;'z', 'y', 'x'&#125;, &#123;'t'&#125;, &#123;'t', 'z'&#125;, &#123;'t', 'y'&#125;, &#123;'t', 'y', 'z'&#125;, &#123;'t', 'x'&#125;, &#123;'t', 'y', 'x'&#125;, &#123;'z', 't', 'x'&#125;, &#123;'z', 't', 'y', 'x'&#125;, &#123;'x'&#125;, &#123;'z', 'x'&#125;, &#123;'z'&#125;] 示例：从新闻网站点击流中挖掘 下列文件kosarak.dat中包含了将近100万条记录。该文件中的每一行包含某个用户浏览过的新闻报道。一些用户只看过一篇报道，而有些用户看过2498篇报道。用户和报道被编码成整数，所以查看频繁项集很难得到更多的东西，但该数据对于展示FP-growth算法的速度十分有效。 数据前几行如下： 12345678910111213141 2 314 5 6 71 89 1011 6 12 13 14 15 161 3 717 1811 6 19 20 21 22 23 241 25 326 311 27 6 3 28 7 29 30 31 32 33 34 35 36 376 2 3839 11 27 1 40 6 41 42 43 44 45 46 47 3 48 7 49 50 51 运用FP-growth算法： 123456789101112131415161718192021222324252627# 导入数据&gt;&gt;&gt; parsedDat = [line.split() for line in open('kosarak.dat').readlines()]# 初始化数据&gt;&gt;&gt; initSet = fpGrowth.createInitSet(parsedDat)# 构建FP树，寻找至少被10万人浏览过的新闻报道&gt;&gt;&gt; myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 100000)# 创建空列表保存频繁项集&gt;&gt;&gt; myFreqList = []&gt;&gt;&gt; fpGrowth.mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreqList)conditional tree for: &#123;'1'&#125; Null Set 1 6 107404conditional tree for: &#123;'3'&#125; Null Set 1 6 186289 11 117401 11 9718conditional tree for: &#123;'11', '3'&#125; Null Set 1 6 117401conditional tree for: &#123;'11'&#125; Null Set 1 6 261773&gt;&gt;&gt; len(myFreqList)9&gt;&gt;&gt; myFreqList[&#123;'1'&#125;, &#123;'1', '6'&#125;, &#123;'3'&#125;, &#123;'11', '3'&#125;, &#123;'11', '6', '3'&#125;, &#123;'6', '3'&#125;, &#123;'11'&#125;, &#123;'11', '6'&#125;, &#123;'6'&#125;] 小结 FP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。Apriori算法产生候选项集，然后扫描数据集来检查它们是否频繁。由于只对数据集扫描两次，因此FP-growth算法执行更快。在FP-growth算法中，数据集存储在FP树中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，直到FP树只包含一个元素为止。 可以使用FP-growth算法在多种文本文档中查找频繁单词。对Twitter源上的某个话题应用FP-growth算法，可以得到一些有关该话题的摘要信息。频繁项集生成还可以用于购物交易、医学诊断和大气研究等。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>Association Analysis</tag>
        <tag>FP-growth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录实用的python库]]></title>
    <url>%2F2018%2F02%2F03%2F%E8%AE%B0%E5%BD%95%E5%AE%9E%E7%94%A8%E7%9A%84python%E5%BA%93%2F</url>
    <content type="text"><![CDATA[本文专门用来记录一下python中一些好用方法/库，可以在日常使用中提高效率。 进度条 在爬虫和机器学习等工作中，可能需要有一个进度条能够反馈当前程序运行速度或者进度，可以考虑用以下方法实现： ShowProcess类 在网上找到别人写的一个方法如下： 1234567891011121314151617181920212223242526# 建立一个ShowProcess类class ShowProcess(): i = 0 max_steps = 0 max_arrow = 50 def __init__(self, max_steps): self.max_steps = max_steps self.i = 0 def show_process(self, i=None): if i is not None: self.i = i else: self.i += 1 num_arrow = int(self.i * self.max_arrow / self.max_steps) num_line = self.max_arrow - num_arrow percent = self.i * 100.0 / self.max_steps process_bar = '[' + '&gt;' * num_arrow + '-' * num_line + ']'\ + '%.2f' % percent + '%' print('\r',process_bar,end='',flush=True) def close(self, words='done'): print('') print(words) self.i = 0 使用示例： 12345678910111213if __name__=='__main__': max_steps = 100 process_bar = ShowProcess(max_steps) for i in range(max_steps): process_bar.show_process() time.sleep(0.05) process_bar.close()# 效果如下 &gt;&gt;&gt; [&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00%&gt;&gt;&gt; done 在命令窗口下使用正常，但是在IDLE和Spyder中显示存在问题，考虑使用其他方法。 tqdm安装1pip install tqdm 使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 最基本的用法import timefrom tqdm import tqdmfor i in tqdm(range(9)): time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:01&lt;00:00, 9.79it/s]# trange类似于tqdmimport timefrom tqdm import trangefor i in trange(10): time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:01&lt;00:00, 9.79it/s]# 传入listimport timefrom tqdm import tqdmpbar = tqdm([1,2,3,4,5,6,7,8,9,10]) for char in pbar: pbar.set_description("Processing %s" % char) time.sleep(0.1)# 效果如下&gt;&gt;&gt; Processing 10: 100%|██████████| 10/10 [00:01&lt;00:00, 9.49it/s] # 手动控制更新import timefrom tqdm import tqdmwith tqdm(total=10) as pbar: for i in range(10): pbar.update(1) time.sleep(0.1)# 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:00&lt;00:00, 10.10it/s]# 也可以这样import timefrom tqdm import tqdmpbar = tqdm(total=10) for i in range(10): pbar.update(1) time.sleep(0.1)pbar.close() # 效果如下&gt;&gt;&gt; 100%|██████████| 10/10 [00:00&lt;00:00, 10.10it/s] 在Spyder下正常了，然而在命令窗口有问题。 重试 进行爬虫的时候，很容易因为网络问题导致失败，这里有2个库可以很轻松地实现这个功能。 retry安装1pip install retry 使用 只需要在函数定义前加上@retry就行了。 1234567891011121314151617from retry import retry@retry()def make_trouble(): '''重试直到成功'''@retry(ZeroDivisionError, tries=3, delay=2)def make_trouble(): '''出现ZeroDivisionError时重试, 重试3次，每次间隔2秒'''@retry((ValueError, TypeError), delay=1, backoff=2)def make_trouble(): '''出现ValueError或TypeError时重试, 每次间隔1, 2, 4, 8, ...秒'''@retry((ValueError, TypeError), delay=1, backoff=2, max_delay=4)def make_trouble(): '''出现ValueError或TypeError时重试, 每次间隔1, 2, 4, 4, ...秒，最高间隔为4秒'''@retry(ValueError, delay=1, jitter=1)def make_trouble(): '''出现ValueError时重试,每次间隔1, 2, 3, 4, ... 秒''' Tenacity 使用类似于retry。同样只需要在函数定义前加上@retry就行了。 安装1pip install tenacity 使用12345678910111213141516171819from tenacity import retry, retry_if_exception_type, wait_fixed, stop_after_attempt, stop_after_delay,@retry()def make_trouble(): '''重试直到成功'''@retry(retry=retry_if_exception_type(ZeroDivisionError), wait=wait_fixed(2), stop=stop_after_attempt(3))def make_trouble(): '''出现ZeroDivisionError时重试, 重试3次，每次间隔2秒'''@retry(stop=(stop_after_delay(10) | stop_after_attempt(5)))def make_trouble(): '''重试10秒或者5次''' @retry(wait=wait_random(min=1, max=2))def make_trouble(): '''重试间隔在随机1-2秒'''@retry(wait=wait_chain(*[wait_fixed(3) for i in range(3)] + [wait_fixed(7) for i in range(2)] + [wait_fixed(9)]))def make_trouble(): '''前三次重试每次间隔3秒，接下来2次间隔7秒，之后重试间隔9秒''' 超时 很多任务特别是多线程时，为了防止程序卡死，需要设定一个超时。 func_timeout 由于windows下signal的支持问题，选择使用第三方包，func_timeout就是一个给函数添加超时的包。 安装1pip install func_timeout 使用123456789101112131415161718192021222324import timefrom func_timeout import func_set_timeout,FunctionTimedOut# 基本用法try: doitReturnValue = func_timeout(5, doit, args=('arg1', 'arg2'))except FunctionTimedOut: print ( "doit('arg1', 'arg2') could not complete within 5 seconds and was terminated.\n")except Exception as e: # 其他Exception# 装饰器用法@func_set_timeout(2)def task(): time.sleep(5)# 效果如下FunctionTimedOut: Function task (args=()) (kwargs=&#123;&#125;) timed out after 2.000000 seconds.# 捕获异常from func_timeout.exceptions import FunctionTimedOuttry: task()except FunctionTimedOut: print('task func_timeout') 此外它还有一个重试的函数FunctionTimedOut，就不赘述了。 异步 待填坑……]]></content>
      <categories>
        <category>心得体会</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金缕曲]]></title>
    <url>%2F2018%2F01%2F31%2F%E9%87%91%E7%BC%95%E6%9B%B2%2F</url>
    <content type="text"><![CDATA[搬运的旧文之二： 今日看微博，居然还有人拿汪精卫的诗词才情给他翻案。其中举了汪的《金缕曲·别后平安否》，原词如下： 12345678别后平安否？便相逢、凄凉万事，不堪回首。国破家亡无穷恨，禁得此生消受。又添了离愁万斗。 眼底心头如昨日，诉心期夜夜常携手。一腔血，为君剖。泪痕料渍云笺透。倚寒衾循环细读，残灯如豆。留此馀生成底事，空令故人僝僽。愧戴却头颅如旧。跋涉关河知不易，愿孤魂缭护车前后。肠已断，歌难又。 说实在话，这首词还是写的挺好的，难怪陈壁君见词倾心。汪另一首著名的诗便是“慷慨歌燕市，从容做楚囚；引刀成一快，不负少年头。”据说当年作出“砍头不要紧，只要主义真；杀了夏明翰，还有后来人。”的革命烈士夏明翰死前是引用了这首诗的，可惜涉及到了大汉奸汪精卫，只能在教科书上删去此段。钱钟书先生读过汪精卫的诗词也不禁叹到：“扫叶吞花足胜情，鉅公难得此才清。”只可惜，汉奸就是汉奸，若是才华便能给人品洗地，那么秦桧的书法也不错，也该给秦桧翻翻案了。 言归正传，汪的《金缕衣·别后平安否》借鉴了顾贞观的《金缕曲二首》： 1234567891011121314151617季子平安否？便归来，平生万事，那堪回首。行路悠悠谁慰藉，母老家贫子幼。记不起、从前杯酒。魑魅搏人应见惯，总输他、覆雨翻云手。冰与雪，周旋久。泪痕莫滴牛衣透，数天涯，依然骨肉，几家能够？比似红颜多命薄，更不如今还有。只绝塞、苦寒难受。廿载包胥承一诺，盼乌头马角终相救。置此札，君怀袖。我亦飘零久。十年来，深恩负尽，死生师友。宿昔齐名非忝窃，试看杜陵消瘦，曾不减、夜郎潺僽。薄命长辞知己别，问人生、到此凄凉否？千万恨，为君剖。兄生辛未吾丁丑，共些时，冰霜摧折，早衰蒲柳。词赋从今须少作，留取心魂相守。但愿得、河清人寿。归日急翻行戍稿，把空名料理传身后。言不尽，观顿首。 顺治十四年，顾贞观的好友吴兆骞参加江南乡试中举，却不幸被牵涉入丁酉江南乡试科场案。顺治帝大怒遂于次年将该科已考中的江南举子押解至北京，由福临在中南海瀛台亲自复试，复试合格者保留举人资格，不合格者治罪。两名主考官被斩，17名同考官处绞。 据载，吴兆骞少年得志，放诞不羁，简傲礼法，曾对江东名士汪琬曰：“江东无我，卿当独步”，当是时汪琬年长他七岁，文名早著，所以引得众人侧目。还有个故事是，他在私塾里念书时，常拿桌上同学们除下来的帽子来小便。同学们告诉先生，他竟有回答：“与其放在俗人头上，还不如拿来盛小便。”先生也不由叹息：“此子必以名大惹祸。”吴兆骞之狂可见一斑。 所以有种说法是因为他太狂傲，故意在复试中交了白卷，因而下狱。于顺治十五年，流放宁古塔（位于今黑龙江省）。 在吴兆骞出关之时，好友顾贞观曾发下誓言，要救他归来。（《无锡金匮县志-文苑》载：“兆骞戍宁古塔，贞观洒涕，要言曰：‘必归季子。‘ ”）然而他奔波近二十年，仍然徒劳无功。康熙十五年冬，离居北京千佛寺，于冰雪中感念良友的惨苦无告，为之作《金缕曲》二首寄之以代书信。纳兰性德读过这两首词，泪下数行，说：“河粱生别之诗，山阳死友之传，得此而三！”当即担保援救兆骞，并回赠一首《金缕曲·赠梁汾》。​ 12345678德也狂生耳！偶然间、淄尘京国，乌衣门第。有酒惟浇赵州土，谁会成生此意？不信道、遂成知己。青眼高歌俱未老，向尊前、拭尽英雄泪。君不见，月如水。共君此夜须沉醉。且由他、娥眉谣诼，古今同忌。身世悠悠何足问，冷笑置之而已！寻思起、从头翻悔。一日心期千劫在，后身缘恐结他生里。然诺重，君须记！ 在纳兰父子的帮助下，吴兆骞终于在康熙二十年后归来。自此他已在塞外生活了二十三年。 注1：我见过有说法，顾的两首《金缕曲》是对吴在塞外写的《戊午二月十一日寄顾舍人书》：“塞外苦寒，四时冰雪，鸣镝呼风，哀笳带雪，一身飘寄，双鬓渐星。妇复多病，一男两女，藜藿不充，回念老母，茕然在堂，迢递关河，归省无日……”的回信，但是该信写于康熙十七年，晚于金缕曲成作日，暂不采纳。 注2：吴伟业在吴兆骞出关时作了一首《悲歌赠吴季子》，我也觉得写的挺好的： 12345678910白璧青蝇见排诋。一朝束缚去，上书难自理。 绝塞千里断行李，送吏泪不止，流人复何倚。 彼尚愁不归，我行定已矣。八月龙沙雪花起， 橐驼垂腰马没耳，白骨皑皑经战垒， 黑河无船渡者几，前忧猛虎后苍， 土穴偷生若蝼蚁，大鱼如山不见尾， 张耆为风沫为雨，日月倒行入海底， 白昼相逢半人鬼。噫嘻乎悲哉! 生男聪明慎莫喜，仓颉夜哭良有以， 受患只从读书始，君不见，吴季子!]]></content>
      <categories>
        <category>诗词遗珠</category>
      </categories>
      <tags>
        <tag>Literature</tag>
        <tag>Poetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[临终歌]]></title>
    <url>%2F2018%2F01%2F31%2F%E4%B8%B4%E7%BB%88%E6%AD%8C%2F</url>
    <content type="text"><![CDATA[庆祝博客开通，搬篇旧文凑个数： 1234临终歌大鹏飞兮振八裔，中天摧兮力不济。余风激兮万世，游扶桑兮挂左袂。后人得之传此，仲尼亡兮谁为出涕？ 李白的好诗有很多，小时候背过的也不少，这里我要说的是《临终歌》。据说是李白死前所作，颇有几分墓志铭的味道——唐代李华于《故翰林学士李君墓铭序》载：“年六十有二不偶，赋临终歌而卒。” 提到大鹏，脑海里自然浮现出了庄子的《逍遥游》：“北冥有鱼，其名为鲲。鲲之大，不知其几千里也。化而为鸟，其名为鹏。鹏之背，不知其几千里也。怒而飞，其翼若垂天之云……”李白自小受道家影响颇深，自然对大鹏也有特别的情感。 年轻时的李白，便作了《大鹏遇希有鸟赋》，赋中李白仿佛化身大鹏，“簸鸿蒙，扇雷霆。斗转而天动，山摇而海倾。”此时的李白尚有“大济苍生，海内清一”的抱负。然而理想与现实终究是有差距的，虽然李白渴望建功立业，却一直不受重用，甚至被玄宗皇帝赐金还山，这时的他依然写下了“大鹏一日同风起，扶摇直上九万里”的诗句，激励自己穿过风浪，东山再起。安史之乱后，李白又受永王李璘叛逆案牵连，流放夜郎。虽然因大赦逃过一劫，但再也没有机会展示抱负了。 同李白的“天生我才必有用”、“吾辈岂是蓬蒿人”、”我本楚狂人，凤歌笑孔丘”等诗句相比，《临终歌》显得尤为悲怆。李白化身的大鹏老了，再也飞不动了，“冠盖满京华，斯人独憔悴”，绝望的大鹏在生命的最后长歌当哭，留下了最后的哀歌。]]></content>
      <categories>
        <category>诗词遗珠</category>
      </categories>
      <tags>
        <tag>Literature</tag>
        <tag>Poetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori算法]]></title>
    <url>%2F2018%2F01%2F30%2FApriori%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[很多人都听说过“尿布和啤酒”的故事：据说，美国中西部的一家连锁店发现，男人们去超市买尿布的同时，往往会顺便给自己购买啤酒。由此，卖场开始把啤酒和尿布摆放在相同区域，让男人可以同时找到这两件商品，从而获得了很好的销售收入。虽然并没有商店真的把这两样东西放在一起，但是很多商家确实将大家经常购买的物品放在一起捆绑销售以鼓励大家购买。那么我们如何在繁杂的数据发现这些隐含关系呢？这就需要关联分析（association analysis），本文所讨论的Apriori便是其中一种关联分析算法。 基本概念 关联分析是一种在大规模数据集中寻找有趣关系的任务。这些关系有两种形式：频繁项集、关联规则。频繁项集（frequent item sets）是经常出现在一块的物品的集合；关联规则（association rules）暗示两种物品之间可能存在很强的关系。 以下是某个杂货店的交易清单： 交易号码 商品 0 豆奶，莴苣 1 莴苣，尿布，葡萄酒，甜菜 2 豆奶，尿布，葡萄酒，橙汁 3 莴苣，豆奶，尿布，葡萄酒 4 莴苣，豆奶，尿布，橙汁 频繁项集：经常出现在一起的物品集合，如｛葡萄酒，尿布，豆奶｝就是一个频繁项集。 支持度（support）：如何有效定义频繁？其中最重要的两个概念是支持度和可信度。一个项集的支持度被定义为数据集中包含该项集的记录所占的比例。还是上面的例子，豆奶在5条交易中出现了4次，因此｛豆奶｝的支持度为4/5，同理可知，｛豆奶，尿布｝的支持度为3/5。我们可以定义一个最小支持度，从而只保留满足最小支持度的项集。 可信度或置信度（confidence）：是针对一条关联规则来定义的。例如：我们要讨论｛尿布｝→｛葡萄酒｝的关联规则，它的可信度被定义为“支持度（｛尿布，葡萄酒｝） / 支持度（｛尿布｝）”。因为｛尿布, 葡萄酒｝的支持度为3/5，｛尿布｝的支持度为4/5，所以“尿布→葡萄酒”的可信度为3/4=0.75。 Apriori原理 假设我们在经营一家商品种类并不多的杂货店，我们对那些经常一起被购买的商品很感兴趣。我们只有4种商品：商品0，商品1，商品2和商品3。那么所有可能被一起购买的商品组合有哪些？下图显示了物品之间所有可能的组合。 如何对一条给定的集合，如{0,3}，来计算其支持度？通常我们遍历每条记录并检查该记录包含0和3，如果记录确实包含两项，那么就增加总计数值。在扫描完每条数据后，使用统计的总数除以总交易记录数，就可以得到支持率。同样地，如果要获得每种可能集合的支持度就要多次重复上述过程。对于包含4种物品的集合，需要遍历数据15次。而随着物品数目的增加，遍历次数会急剧增长。对于包含N中物品的数据集共有2N−1中项集组合，对于只出售100中商品的商店也会有1.26×1030中可能的项集组合。对于现代计算机，需要很长的时间才能完成运算。 Apriori原理可以帮助我们减少感兴趣的项集。Apriori原理是指如果某个项集是频繁的，那么它的所有子集也是频繁的。反过来，如果一个项集是非频繁集，那么它的所有超集也是非频繁的。 上述例子中，已知阴影项集{2，3}是非频繁的。利用这个知识，我们就知道项集{0,2,3} ，{1,2,3}以及{0,1,2,3}也是非频繁的。这也就是说，一旦计算出了{2,3}的支持度，知道它是非频繁的之后，就不需要再计算{0,2,3}、{1,2,3}和{0,1,2,3}的支持度，因为我们知道这些集合不会满足我们的要求。使用该原理就可以避免项集数目的指数增长，从而在合理时间内计算出频繁项集。 Apriori算法 发现频繁项集的过程如上图所示： 由数据集生成候选项集C1（1表示每个候选项仅有一个数据项）；再由C1通过支持度过滤，生成频繁项集L1（1表示每个频繁项仅有一个数据项）。 将L1的数据项两两拼接成C2。 从候选项集C2开始，通过支持度过滤生成L2。L2根据Apriori原理拼接成候选项集C3；C3通过支持度过滤生成L3……直到Lk中仅有一个或没有数据项为止。 回到上面的杂货店例子，令最小支持度为0.4，结果如下图： 值得注意的是L3到C4这一步并没有得到候选项集，这是由于Apriori算法由两部分组成（在这里假定购买商品是有顺序的）。 连接：对K-1项集中的每个项集中的项排序，只有在前K-1项相同时才将这两项合并，形成候选K项集（因为必须形成K项集，所以只有在前K-1项相同，第K项不相同的情况下才合并。） 剪枝：对于候选K项集，要验证所有项集的所有K-1子集是否频繁（是否在K-1项集中），去掉不满足的项集，就形成了K项集。比如C4连接的｛尿布，莴苣，葡萄酒，豆奶｝的子集｛莴苣，葡萄酒，豆奶｝不存在于L3，因此要去掉。 实现Apriori代码 根据以上原理构造数据集扫描的Python代码，其伪代码大致如下： 1234567对数据集中的每条交易记录tran对每个候选项集can : 检查一下can是否是tran的子集 : 如果是，则增加can的计数值对每个候选项集 :如果其支持度不低于最小值，则保留该项集返回所有频繁项集列表 建立辅助函数： 12345678910111213141516171819202122232425262728293031323334353637383940# 创建一个简单的测试数据集def loadDataSet() : return [[1,3,4], [2,3,5], [1,2,3,5], [2,5]]# 构建集合C1，C1是大小为1的所有候选项集的集合。def createC1(dataSet) : # C1是空列表，用来存储所有不重复的项值。如果某个物品项没有在C1中出现，则将其添加到C1中。 # 这里并不是简单地每个物品项，而是添加只包含该物品项的一个列表。Python不能创建只有一个整 # 数的集合，因此这里实现必须使用列表 C1 = [] for transaction in dataSet : for item in transaction : if not [item] in C1 : C1.append([item]) C1.sort() # frozenset是指被“冰冻”的集合，就是说它们是不可改变 return list(map(frozenset,C1)) # D: 数据集# Ck: 候选项集列表# minSupport: 感兴趣集的最小支持度minSupport# 该函数会返回一个包含支持度的字典以备后用def scanD(D, Ck, minSupport) : ssCnt = &#123;&#125; for tid in D : for can in Ck : if can.issubset(tid) : if not can in ssCnt: ssCnt[can]=1 else : ssCnt[can] += 1 numItems = float(len(D)) retList = [] supportData = &#123;&#125; for key in ssCnt : # 计算所有项集的支持度 support = ssCnt[key]/numItems if support &gt;= minSupport : # 在列表的首部插入新的集合 retList.insert(0, key) supportData[key] = support return retList, supportData 保存为apriori.py，运行效果如下： 123456789101112131415161718&gt;&gt;&gt; import apriori# 导入数据集&gt;&gt;&gt; dataSet = apriori.loadDataSet()&gt;&gt;&gt; dataSet[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]# 构建第一个候选项集集合C1&gt;&gt;&gt; C1 = apriori.createC1(dataSet)&gt;&gt;&gt; C1[frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])]# 构建集合表示的数据集D&gt;&gt;&gt; D = list(map(set, dataSet))&gt;&gt;&gt; D[&#123;1, 3, 4&#125;, &#123;2, 3, 5&#125;, &#123;1, 2, 3, 5&#125;, &#123;2, 5&#125;]# 去掉不满足最小支持度的项集，0.5为最小支持度&gt;&gt;&gt; L1, suppData0 = apriori.scanD(D, C1, 0.5)# 下面四个项集构成了L1列表，该列表中每个单物品项集至少出现在50%以上的记录中&gt;&gt;&gt; L1[frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])] 整个Apriori算法的伪代码如下： 1234当集合中项的个数大于0时 构建一个k个项组成的候选项集的列表 检查数据以确认每个项集都是频繁的 保留频繁项集并构建k+1项组成的候选项集的列表 将如下算法代码加入apriori.py： 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建候选项集Ck# Lk，频繁项集列表# k，项集元素的个数def aprioriGen(Lk, k) : # create Ck # 创建一个空列表 retList = [] # 计算Lk中的元素 lenLk = len(Lk) for i in range(lenLk) : for j in range(i+1, lenLk) : # 当前k-2个项相同时，将两个集合合并 L1 = list(Lk[i])[:k-2] L2 = list(Lk[j])[:k-2] L1.sort() L2.sort() if L1==L2 : # python中集合的并操作对应的操作符为| retList.append(Lk[i] | Lk[j]) return retList# dataSet，数据集# minSupport，支持度# 此函数会生成候选项集的列表def apriori(dataSet, minSupport = 0.5) : C1 = createC1(dataSet) # map函数将set()映射到dataSet列表中的每一项 D = list(map(set, dataSet)) L1, supportData = scanD(D, C1, minSupport) # 将L1放入L列表中 L = [L1] k = 2 # while循环将L2, L3, L4, ... 放入L列表中，直到下一个大的项集为空 while (len(L[k-2]) &gt; 0) : # 调用aprioriGen()创建候选项集Ck Ck = aprioriGen(L[k-2], k) # 扫描数据集，从Ck得到Lk Lk, supK = scanD(D, Ck, minSupport) supportData.update(supK) L.append(Lk) k += 1 return L, supportData 上面的k-2可能会令人困惑，接下来讨论其细节。当利用{0}、{1}、{2}构建{0,1}、{0,2}、{1,2}时，实际上是将单个项组合到一块。现在如果想利用{0,1}、{0,2}、{1,2}来创建三元素项集，应该怎么做？如果将每两个集合合并，就会得到{0,1,2}、{0,1,2}、{0,1,2}。也就是同样的结果集合会重复3次。接下来需要扫描三元素项集列表来得到非重复结果，我们要做的是确保遍历列表的次数最少。现在，如果比较集合{0,1}、{0,2}、{1,2}的第一个元素并只对第一个元素相同的集合求并操作，又会得到什么结果？{0,1,2}，而且只有一次操作，这样就不用遍历列表来寻找非重复值了。 保存后运行效果如下： 1234567891011121314151617181920212223&gt;&gt;&gt; L, supportData = apriori.apriori(dataSet)&gt;&gt;&gt; L[[frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])], [frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5]), frozenset([1, 3])], [frozenset([2,3, 5])], []]# L包含满足最小支持度为0.5的频率项集列表，下面看一下具体值：&gt;&gt;&gt; L[0][frozenset([5]), frozenset([2]), frozenset([3]), frozenset([1])]&gt;&gt;&gt; L[1][frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5]), frozenset([1, 3])]&gt;&gt;&gt; L[2][frozenset([2, 3, 5])]&gt;&gt;&gt; L[3][]# 每个项集都是在函数apriori()中调用函数aprioriGen()来生成的。下面看一下aprioriGen()函数的工作流程：&gt;&gt;&gt; apriori.aprioriGen(L[0], 2)[frozenset([2, 5]), frozenset([3, 5]), frozenset([1, 5]), frozenset([2, 3]), frozenset([1, 2]), frozenset([1, 3])]# 这里的6个集合是候选项集Ck中的元素。其中4个集合在L[1]中，剩下2个集合被函数scanD()过滤掉。# 下面再尝试70%的支持度：&gt;&gt;&gt; L,support = apriori.apriori(dataSet, minSupport=0.7)&gt;&gt;&gt; L[[frozenset([5]), frozenset([2]), frozenset([3])], [frozenset([2, 5])], []] 从频繁项集中挖掘关联规则 关联分析的两个重要目标是发现频繁项集与关联规则。要找到关联规则，首先从一个频繁项集开始，集合中的元素是不重复的，但我们想知道基于这些元素能否获得其他内容。某个元素或者某个元素集合可能会推导出另一个元素。例如，一个频繁项集｛豆奶, 莴苣｝，可能有一条关联规则“豆奶→莴苣”，这意味着如果有人购买了豆奶，那么在统计上他购买莴苣的概率较大。但是这条反过来并不总是成立。换言之，即使“豆奶→莴苣”统计上显著，那么“莴苣→豆奶”也不一定成立。箭头的左边集合称作前件，箭头右边的集合称为后件。 上节我们给出了繁琐项集的量化定义，即它满足最小支持度要求。对于关联规则，我们也有类似量化方法，这种量化标准称为可信度。一条规则P→H的可信度定义为support(P | H) / support(P)。在前面我们已经计算了所有繁琐项集支持度，要想获得可信度，只需要再做一次除法运算。 从一个繁琐项集中可以产生多少条关联规则？下图给出了从项集{0,1,2,3}产生的所有关联规则。为了找到感兴趣的规则，我们先生成一个可能的规则列表，然后测试每条规则可信度。如果可信度不满足最小要求，则去掉该规则。 可以观察到，如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求。具体而言，如果012→3是一条低可信度规则，则所有其它3为后件的规则都是低可信度。这需要从可信度的概念去理解，Confidence(012→3) = P(3|0,1,2), Confidence(01→23)=P(2,3|0,1)，P(3|0,1,2) &gt;= P(2,3|0,1)。由此可以对关联规则做剪枝处理。 还是以之前的杂货店交易数据为例，我们发现了以下频繁项集： 对于寻找关联规则来说，频繁1项集L1没有用处，因为L1中的每个集合仅有一个数据项，至少有两个数据项才能生成A→B这样的关联规则。取置信度为0.7，最终从L2发掘出10条关联规则： 接下来是L3： 假设有一个L4项集（文中的数据不能生成L4），其挖掘过程如下： 利用此性质来减少测试的规则数目，可以先从一个频繁项集开始，接着创建一个规则列表，其中规则右部只包含一个元素，然后对这些规则测试。接下来合并所有剩余规则来创建一个新的规则列表，其中右部包含两个元素。这种方法称为分级法。打开apriori.py，加入如下代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 关联规则生成函数，此函数调用其他两个函数rulesFromConseq、calcConf# L: 频繁项集列表# supportData: 包含那些频繁项集支持数据的字典# minConf: 最小可信度阈值，默认是0.7# 函数最后要生成一个包含可信度的规则列表，后面可以基于可信度对它们进行排序# 这些规则存放在bigRuleList中。def generateRules(L, supportData, minConf=0.7) : bigRuleList = [] # 遍历L中的每一个频繁项集并对每个频繁项集创建只包含单个元素集合的列表H1， # 因为无法从单元素项集中构建关联规则，所以要从包含两个或者更多元素的项集开始规则构建过程。 # 只获取有两个或更多元素的集合 for i in range(1, len(L)) : for freqSet in L[i] : H1 = [frozenset([item]) for item in freqSet] if i &gt; 1 : # 如果频繁项集的元素数目超过2，那么会考虑对它做进一步的合并，合并通过 # rulesFromConseq来完成 rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else : # 如果项集中只有两个元素，那么需要使用calcConf()来计算可信度值 calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleList# 对规则进行评估# 目标是计算规则的可信度以及找到满足最小可信度要求的规则# 函数会返回一个满足最小可信度要求的规则列表，空列表prunedH保存这些规则def calcConf(freqSet, H, supportData, brl, minConf=0.7) : prunedH = [] # 遍历H中的所有项集并计算它们的可信度值 for conseq in H : # 可信度计算时使用supportData中的支持度数据 conf = supportData[freqSet] / supportData[freqSet - conseq] # 规则满足最小可信度值，将这些规则输出到屏幕显示 if conf &gt;= minConf : print(freqSet-conseq, '--&gt;', conseq, 'conf:', conf) brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedH# 用于生成候选规则集合，从最初的项集中生成更多的关联规则# freqSet: 频繁项集# H: 可以出现在规则右部的元素列表def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7) : # H中频繁项集大小m m = len(H[0]) # 查看该频繁项集是否大到可以移除大小为m的子集 if (len(freqSet) &gt; (m+1)) : # 生成H中元素的无重复组合，结果存储在Hmp1，这也是下一次迭代的H列表 Hmp1 = aprioriGen(H, m+1) # Hmp1包含所有可能的规则，利用calcConf()来测试它们的可信度以确定是否满足要求 Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf) # 如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq if (len(Hmp1) &gt; 1) : rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf) 检验运行效果： 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; import apriori&gt;&gt;&gt; dataSet = apriori.loadDataSet()&gt;&gt;&gt; dataSet[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]# 生成一个最小支持度为0.5的频繁项集的集合&gt;&gt;&gt; L, supportData = apriori.apriori(dataSet, minSupport=0.5)&gt;&gt;&gt; rules = apriori.generateRules(L, supportData, minConf=0.7)frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0frozenset([1]) --&gt; frozenset([3]) conf: 1.0&gt;&gt;&gt; rules[(frozenset([5]), frozenset([2]), 1.0), (frozenset([2]), frozenset([5]), 1.0), (frozenset([1]), frozenset([3]), 1.0)]# 得到了3条规则：&#123;5&#125;→&#123;2&#125;、&#123;2&#125;→&#123;5&#125;、&#123;1&#125;→&#123;3&#125;，可见包含2和5的规则可以互换前后件，包含1和3的不行# 接下来降低可信度阈值，可以得到结果如下&gt;&gt;&gt; rules = apriori.generateRules(L, supportData, minConf=0.5)frozenset([3]) --&gt; frozenset([2]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([5]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0frozenset([3]) --&gt; frozenset([1]) conf: 0.666666666667frozenset([1]) --&gt; frozenset([3]) conf: 1.0frozenset([5]) --&gt; frozenset([2, 3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([2, 5]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3, 5]) conf: 0.666666666667&gt;&gt;&gt; rules[(frozenset(&#123;3&#125;), frozenset(&#123;2&#125;), 0.6666666666666666), (frozenset(&#123;2&#125;), frozenset(&#123;3&#125;), 0.6666666666666666), (frozenset(&#123;5&#125;), frozenset(&#123;3&#125;), 0.6666666666666666), (frozenset(&#123;3&#125;), frozenset(&#123;5&#125;), 0.6666666666666666), (frozenset(&#123;5&#125;), frozenset(&#123;2&#125;), 1.0), (frozenset(&#123;2&#125;), frozenset(&#123;5&#125;), 1.0), (frozenset(&#123;3&#125;), frozenset(&#123;1&#125;), 0.6666666666666666), (frozenset(&#123;1&#125;), frozenset(&#123;3&#125;), 1.0), (frozenset(&#123;5&#125;), frozenset(&#123;2, 3&#125;), 0.6666666666666666), (frozenset(&#123;3&#125;), frozenset(&#123;2, 5&#125;), 0.6666666666666666), (frozenset(&#123;2&#125;), frozenset(&#123;3, 5&#125;), 0.6666666666666666)]# 一旦降低可信度阈值，就可以获得更多的规则 Apriori应用 之前我们在小数据上应用了apriori算法，接下来要在更大的真实数据集上测试效果。那么可以使用什么样的数据呢？比如：购物篮分析，搜索引擎的查询词，国会投票，毒蘑菇的相似特征提取等； 示例：发现毒蘑菇的相似特征 从此处下载mushroom.dat，其前几行如下： 12345671 3 9 13 23 25 34 36 38 40 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 14 23 26 34 36 39 40 52 55 59 63 67 76 85 86 90 93 99 108 114 2 4 9 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 108 115 1 3 10 15 23 25 34 36 38 41 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 16 24 28 34 37 39 40 53 54 59 63 67 76 85 86 90 94 99 109 114 2 3 10 14 23 26 34 36 39 41 52 55 59 63 67 76 85 86 90 93 98 108 114 2 4 9 15 23 26 34 36 39 42 52 55 59 63 67 76 85 86 90 93 98 108 115 第一个特征表示有毒或者可食用，有毒为2，无毒为1。下一个特征是蘑菇伞的形状，有六种可能的值，分别用整数3-8来表示。 为了找到毒蘑菇中存在的公共特征，可以运行Apriori算法来寻找包含特征值为2的频繁项集。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt;&gt;&gt; import apriori&gt;&gt;&gt; mushDatSet = [line.split() for line in open('mushroom.dat').readlines()]# 在数据集上运行Apriori算法&gt;&gt;&gt; L, suppData = apriori.apriori(mushDatSet, minSupport=0.3)# 在结果中可以搜索包含有独特征2的频繁项集：&gt;&gt;&gt; for item in L[1] :... if item.intersection('2'): print(item)...frozenset(&#123;'28', '2'&#125;)frozenset(&#123;'2', '53'&#125;)frozenset(&#123;'2', '23'&#125;)frozenset(&#123;'2', '34'&#125;)frozenset(&#123;'2', '36'&#125;)frozenset(&#123;'59', '2'&#125;)frozenset(&#123;'63', '2'&#125;)frozenset(&#123;'67', '2'&#125;)frozenset(&#123;'2', '76'&#125;)frozenset(&#123;'2', '85'&#125;)frozenset(&#123;'2', '86'&#125;)frozenset(&#123;'2', '90'&#125;)frozenset(&#123;'93', '2'&#125;)frozenset(&#123;'2', '39'&#125;)# 对更大项集来重复上述过程&gt;&gt;&gt; for item in L[3] :... if item.intersection('2') : print(item)...frozenset(&#123;'2', '28', '59', '34'&#125;)frozenset(&#123;'2', '28', '59', '85'&#125;)frozenset(&#123;'2', '86', '28', '59'&#125;)frozenset(&#123;'2', '28', '59', '90'&#125;)frozenset(&#123;'2', '28', '59', '39'&#125;)frozenset(&#123;'63', '2', '28', '39'&#125;)frozenset(&#123;'63', '2', '28', '34'&#125;)frozenset(&#123;'63', '2', '28', '59'&#125;)frozenset(&#123;'63', '2', '28', '85'&#125;)frozenset(&#123;'63', '2', '86', '28'&#125;)frozenset(&#123;'2', '28', '85', '34'&#125;)frozenset(&#123;'2', '86', '28', '34'&#125;)frozenset(&#123;'2', '86', '28', '85'&#125;)frozenset(&#123;'34', '2', '28', '90'&#125;)frozenset(&#123;'2', '28', '85', '90'&#125;)frozenset(&#123;'2', '86', '28', '90'&#125;)frozenset(&#123;'2', '28', '34', '39'&#125;)frozenset(&#123;'2', '28', '85', '39'&#125;)frozenset(&#123;'2', '86', '28', '39'&#125;)frozenset(&#123;'2', '28', '90', '39'&#125;)frozenset(&#123;'34', '2', '28', '53'&#125;)frozenset(&#123;'2', '28', '85', '53'&#125;)frozenset(&#123;'2', '86', '28', '53'&#125;)frozenset(&#123;'90', '2', '28', '53'&#125;)frozenset(&#123;'2', '28', '53', '39'&#125;)...... 接下来你需要观察这些特征，以便知道蘑菇的各个方面，如果看到其中任何一个特征，那么这些蘑菇就很有可能有毒。 小结 关联特征是用于发现大数据集元素间有趣关系的一个工具集，可以采用两种方法来量化这些有趣关系。第一种方法是使用频繁项集，它会给出经常在一起出现的元素项。第二种方式是关联规则，每条关联规则意味着元素之前的“如果……那么”关系。 发现元素间不同的组合是个非常耗时的任务，不可避免需要大量昂贵的计算资源，这就需要一些更智能的方法在合适的时间范围内找到频繁项集。其中一个方法是Apriori算法，它使用Apriori原理减少在数据库上进行检查的集合的数目。Apriori原理是说如果一个元素项是不频繁的，那么那些包含该元素的超集也是不频繁的。Apriori算法从单元项集开始，通过组合满足最小支持度要求的项集来形成更大的集合。支持度用来度量一个集合在原始数据中出现的频率。 关联分析可以用在许多不同物品上。商店中的商品以及网站的访问页面是其中比较常见的例子。关联分析也曾用于查看选举人及法官的投票历史。 缺点是每次增加频繁项集的大小，Apriori算法都会重新扫描整个数据集，当数据集很大时，这会显著降低频繁项集发现的速度。 参考资料：《机器学习实战》、《数据挖掘导论》]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
        <tag>Association Analysis</tag>
        <tag>Apriori</tag>
      </tags>
  </entry>
</search>
